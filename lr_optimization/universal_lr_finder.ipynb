{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfcab7c",
   "metadata": {},
   "source": [
    "# üîç Universal Learning Rate Finder Toolkit\n",
    "\n",
    "**A comprehensive, model-agnostic learning rate optimization toolkit for any PyTorch model**\n",
    "\n",
    "## üéØ What You'll Learn:\n",
    "- **Multiple LR Finding Methods**: Linear, Exponential, and Cyclical approaches\n",
    "- **Universal Compatibility**: Works with any PyTorch model architecture\n",
    "- **Visual Comparisons**: Side-by-side method comparison and analysis\n",
    "- **Optimal LR Suggestions**: Automated recommendations with explanations\n",
    "- **Production Ready**: Organized structure for real projects\n",
    "\n",
    "## üöÄ Key Features:\n",
    "- ‚úÖ **4 Different LR Finding Methods** - Compare multiple approaches\n",
    "- ‚úÖ **Any Model Support** - CNN, RNN, Transformer, Custom architectures\n",
    "- ‚úÖ **Beautiful Visualizations** - Interactive plots and comparisons\n",
    "- ‚úÖ **Smart Recommendations** - Automated optimal LR detection\n",
    "- ‚úÖ **Organized Results** - Structured saving and experiment tracking\n",
    "- ‚úÖ **Step-by-Step Explanations** - Learn the theory behind each method\n",
    "\n",
    "---\n",
    "\n",
    "Let's start building your learning rate optimization toolkit! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f24838",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Create Subfolder Structure\n",
    "\n",
    "First, let's create an organized directory structure for our LR finding experiments. This will help us keep track of different models, results, and visualizations.\n",
    "\n",
    "### Directory Structure:\n",
    "```\n",
    "lr_optimization/\n",
    "‚îú‚îÄ‚îÄ experiments/          # Experiment results and logs\n",
    "‚îú‚îÄ‚îÄ models/              # Saved model checkpoints\n",
    "‚îú‚îÄ‚îÄ plots/               # Generated visualizations\n",
    "‚îú‚îÄ‚îÄ lr_finder_results/   # Raw LR finder data\n",
    "‚îî‚îÄ‚îÄ configs/             # Configuration files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Create the main directory structure\n",
    "def create_lr_finder_structure():\n",
    "    \"\"\"\n",
    "    Create organized directory structure for LR finding experiments\n",
    "    \"\"\"\n",
    "    base_dir = Path.cwd()\n",
    "    \n",
    "    # Define subdirectories\n",
    "    subdirs = [\n",
    "        'experiments',\n",
    "        'models', \n",
    "        'plots',\n",
    "        'lr_finder_results',\n",
    "        'configs'\n",
    "    ]\n",
    "    \n",
    "    created_dirs = []\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        dir_path = base_dir / subdir\n",
    "        dir_path.mkdir(exist_ok=True)\n",
    "        created_dirs.append(str(dir_path))\n",
    "        print(f\"‚úÖ Created/Verified: {dir_path}\")\n",
    "    \n",
    "    # Create a config file for this session\n",
    "    session_config = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'base_directory': str(base_dir),\n",
    "        'subdirectories': created_dirs,\n",
    "        'session_info': {\n",
    "            'purpose': 'Universal Learning Rate Finder Toolkit',\n",
    "            'methods': ['Linear LR Range Test', 'Exponential LR Range Test', 'Cyclical LR Range Test'],\n",
    "            'features': ['Model-agnostic', 'Comparative analysis', 'Automated recommendations']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = base_dir / 'configs' / 'session_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(session_config, f, indent=2)\n",
    "    \n",
    "    print(f\"üìã Session config saved to: {config_path}\")\n",
    "    return base_dir, created_dirs\n",
    "\n",
    "# Create the directory structure\n",
    "BASE_DIR, CREATED_DIRS = create_lr_finder_structure()\n",
    "\n",
    "print(f\"\\nüéâ LR Finder toolkit structure created successfully!\")\n",
    "print(f\"üìÇ Base directory: {BASE_DIR}\")\n",
    "print(f\"üìÅ Ready for experiments with {len(CREATED_DIRS)} organized folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54486b8",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Let's import all the essential libraries we'll need for our learning rate finding toolkit. We'll include PyTorch for deep learning, visualization libraries, and utilities for data handling.\n",
    "\n",
    "### Key Libraries:\n",
    "- **PyTorch**: Core deep learning framework\n",
    "- **Matplotlib/Seaborn**: Visualization and plotting\n",
    "- **NumPy**: Numerical computations\n",
    "- **Pandas**: Data manipulation and analysis\n",
    "- **tqdm**: Progress bars for training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dac9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CyclicLR\n",
    "\n",
    "# Data handling and computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional libraries (with fallbacks)\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    PLOTLY_AVAILABLE = True\n",
    "    print(\"‚úÖ Plotly available - Interactive plots enabled\")\n",
    "except ImportError:\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Plotly not available - Using matplotlib only\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducible results\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "print(f\"üêç PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Ready for learning rate optimization experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552aae0",
   "metadata": {},
   "source": [
    "## 3. Define Base LR Finder Class\n",
    "\n",
    "Now we'll create a base class that provides common functionality for all our learning rate finding methods. This class will handle:\n",
    "\n",
    "- **Model preparation**: Setting up the model for LR finding\n",
    "- **Loss tracking**: Recording loss values during experiments\n",
    "- **Data management**: Storing and organizing results\n",
    "- **Common utilities**: Shared functions across all methods\n",
    "\n",
    "### Key Features:\n",
    "- üîß **Model-agnostic**: Works with any PyTorch model\n",
    "- üìä **Automatic tracking**: Loss, LR, and gradient monitoring\n",
    "- üíæ **Result storage**: Organized data saving\n",
    "- üéØ **Optimal LR detection**: Smart recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLRFinder:\n",
    "    \"\"\"\n",
    "    Base Learning Rate Finder class with common functionality\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer_class=optim.Adam, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the LR Finder\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch model (any architecture)\n",
    "            criterion: Loss function (e.g., nn.CrossEntropyLoss())\n",
    "            optimizer_class: Optimizer class (default: Adam)\n",
    "            device: Device to run on (auto-detected if None)\n",
    "        \"\"\"\n",
    "        self.model = model.to(device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.device = self.model.parameters().__next__().device\n",
    "        \n",
    "        # Storage for results\n",
    "        self.results = {\n",
    "            'learning_rates': [],\n",
    "            'losses': [],\n",
    "            'gradients': [],\n",
    "            'smooth_losses': [],\n",
    "            'method': '',\n",
    "            'optimal_lr': None,\n",
    "            'optimal_lr_reason': '',\n",
    "            'experiment_info': {}\n",
    "        }\n",
    "        \n",
    "        # Model state management\n",
    "        self.initial_state = None\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        print(f\"üîß LR Finder initialized for {model.__class__.__name__} on {self.device}\")\n",
    "    \n",
    "    def _prepare_model(self):\n",
    "        \"\"\"Prepare model for LR finding experiment\"\"\"\n",
    "        # Save initial model state\n",
    "        self.initial_state = copy.deepcopy(self.model.state_dict())\n",
    "        self.model.train()\n",
    "        return self.model\n",
    "    \n",
    "    def _restore_model(self):\n",
    "        \"\"\"Restore model to initial state\"\"\"\n",
    "        if self.initial_state is not None:\n",
    "            self.model.load_state_dict(self.initial_state)\n",
    "            print(\"üîÑ Model state restored to initial condition\")\n",
    "    \n",
    "    def _calculate_smooth_loss(self, beta=0.98):\n",
    "        \"\"\"Calculate exponentially smoothed loss\"\"\"\n",
    "        if len(self.results['losses']) == 0:\n",
    "            return []\n",
    "        \n",
    "        smooth_losses = []\n",
    "        smooth_loss = self.results['losses'][0]\n",
    "        \n",
    "        for loss in self.results['losses']:\n",
    "            smooth_loss = beta * smooth_loss + (1 - beta) * loss\n",
    "            smooth_losses.append(smooth_loss / (1 - beta ** len(smooth_losses)))\n",
    "        \n",
    "        return smooth_losses\n",
    "    \n",
    "    def _get_gradient_norm(self):\n",
    "        \"\"\"Calculate the norm of gradients\"\"\"\n",
    "        total_norm = 0.0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        return total_norm ** (1. / 2)\n",
    "    \n",
    "    def _should_stop_early(self, current_loss, patience=5, threshold=4.0):\n",
    "        \"\"\"\n",
    "        Determine if training should stop early due to exploding loss\n",
    "        \n",
    "        Args:\n",
    "            current_loss: Current batch loss\n",
    "            patience: Number of steps to wait before stopping\n",
    "            threshold: Multiplier for loss explosion detection\n",
    "        \"\"\"\n",
    "        if len(self.results['smooth_losses']) < patience:\n",
    "            return False\n",
    "        \n",
    "        # Check if loss has exploded compared to best loss\n",
    "        if current_loss > threshold * self.best_loss:\n",
    "            return True\n",
    "        \n",
    "        # Update best loss\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _find_optimal_lr_simple(self):\n",
    "        \"\"\"\n",
    "        Simple method to find optimal learning rate\n",
    "        Uses the point of steepest descent before loss explosion\n",
    "        \"\"\"\n",
    "        if len(self.results['smooth_losses']) < 10:\n",
    "            return self.results['learning_rates'][len(self.results['losses'])//2], \"Middle of range (insufficient data)\"\n",
    "        \n",
    "        smooth_losses = self.results['smooth_losses']\n",
    "        learning_rates = self.results['learning_rates']\n",
    "        \n",
    "        # Find the steepest descent point\n",
    "        gradients = np.gradient(smooth_losses)\n",
    "        min_gradient_idx = np.argmin(gradients)\n",
    "        \n",
    "        # Alternative: Find minimum loss point\n",
    "        min_loss_idx = np.argmin(smooth_losses)\n",
    "        \n",
    "        # Choose the earlier point (more conservative)\n",
    "        optimal_idx = min(min_gradient_idx, min_loss_idx)\n",
    "        \n",
    "        # Safety: Don't pick from the very beginning or end\n",
    "        optimal_idx = max(2, min(optimal_idx, len(learning_rates) - 3))\n",
    "        \n",
    "        optimal_lr = learning_rates[optimal_idx]\n",
    "        reason = f\"Steepest descent at step {optimal_idx} (loss: {smooth_losses[optimal_idx]:.4f})\"\n",
    "        \n",
    "        return optimal_lr, reason\n",
    "    \n",
    "    def save_results(self, filename_prefix=\"lr_finder\"):\n",
    "        \"\"\"Save results to files\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Prepare data for saving\n",
    "        df = pd.DataFrame({\n",
    "            'learning_rate': self.results['learning_rates'],\n",
    "            'loss': self.results['losses'],\n",
    "            'smooth_loss': self.results['smooth_losses'],\n",
    "            'gradient_norm': self.results['gradients']\n",
    "        })\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = BASE_DIR / 'lr_finder_results' / f\"{filename_prefix}_{self.results['method']}_{timestamp}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'method': self.results['method'],\n",
    "            'optimal_lr': self.results['optimal_lr'],\n",
    "            'optimal_lr_reason': self.results['optimal_lr_reason'],\n",
    "            'experiment_info': self.results['experiment_info'],\n",
    "            'timestamp': timestamp,\n",
    "            'total_steps': len(self.results['learning_rates']),\n",
    "            'lr_range': {\n",
    "                'min': min(self.results['learning_rates']) if self.results['learning_rates'] else None,\n",
    "                'max': max(self.results['learning_rates']) if self.results['learning_rates'] else None\n",
    "            },\n",
    "            'loss_range': {\n",
    "                'min': min(self.results['losses']) if self.results['losses'] else None,\n",
    "                'max': max(self.results['losses']) if self.results['losses'] else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        json_path = BASE_DIR / 'lr_finder_results' / f\"{filename_prefix}_{self.results['method']}_metadata_{timestamp}.json\"\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Results saved:\")\n",
    "        print(f\"   üìä Data: {csv_path}\")\n",
    "        print(f\"   üìã Metadata: {json_path}\")\n",
    "        \n",
    "        return csv_path, json_path\n",
    "\n",
    "print(\"‚úÖ Base LR Finder class defined successfully!\")\n",
    "print(\"üîß Features: Model-agnostic, automatic tracking, optimal LR detection\")\n",
    "print(\"üìä Ready to implement specific LR finding methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea8b3d",
   "metadata": {},
   "source": [
    "## 4. Implement Linear LR Range Test\n",
    "\n",
    "The **Linear LR Range Test** is one of the most popular methods for finding optimal learning rates. It works by:\n",
    "\n",
    "1. **Starting small**: Begin with a very small learning rate (e.g., 1e-7)\n",
    "2. **Linear increase**: Gradually increase LR linearly over batches\n",
    "3. **Monitor loss**: Track how loss changes with increasing LR\n",
    "4. **Find sweet spot**: Identify the LR where loss decreases fastest\n",
    "\n",
    "### When to use:\n",
    "- ‚úÖ **First time** with a new model architecture\n",
    "- ‚úÖ **Stable training** - when you want conservative estimates\n",
    "- ‚úÖ **Understanding behavior** - see how model responds to LR changes\n",
    "\n",
    "### Theory:\n",
    "- **Too low LR**: Loss decreases very slowly\n",
    "- **Optimal LR**: Loss decreases rapidly (steepest descent)\n",
    "- **Too high LR**: Loss starts increasing or becomes unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLRFinder(BaseLRFinder):\n",
    "    \"\"\"\n",
    "    Linear Learning Rate Range Test\n",
    "    \n",
    "    Increases learning rate linearly from min_lr to max_lr over num_batches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer_class=optim.Adam, device=None):\n",
    "        super().__init__(model, criterion, optimizer_class, device)\n",
    "        self.results['method'] = 'Linear_LR_Range_Test'\n",
    "    \n",
    "    def find_lr(self, train_loader, min_lr=1e-7, max_lr=10.0, num_batches=None, \n",
    "                stop_div_factor=4.0, smooth_factor=0.98):\n",
    "        \"\"\"\n",
    "        Find optimal learning rate using linear range test\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            min_lr: Minimum learning rate to test\n",
    "            max_lr: Maximum learning rate to test  \n",
    "            num_batches: Number of batches to test (default: full epoch)\n",
    "            stop_div_factor: Stop if loss > stop_div_factor * best_loss\n",
    "            smooth_factor: Smoothing factor for loss (0.98 = strong smoothing)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results with optimal LR and loss curves\n",
    "        \"\"\"\n",
    "        print(f\"üîç Starting Linear LR Range Test...\")\n",
    "        print(f\"üìä LR Range: {min_lr:.2e} ‚Üí {max_lr:.2e}\")\n",
    "        \n",
    "        # Prepare model and reset results\n",
    "        self._prepare_model()\n",
    "        self.results = {key: [] for key in ['learning_rates', 'losses', 'gradients', 'smooth_losses']}\n",
    "        self.results['method'] = 'Linear_LR_Range_Test'\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Determine number of batches\n",
    "        if num_batches is None:\n",
    "            num_batches = len(train_loader)\n",
    "        num_batches = min(num_batches, len(train_loader))\n",
    "        \n",
    "        print(f\"üéØ Testing over {num_batches} batches\")\n",
    "        \n",
    "        # Create optimizer with minimum LR\n",
    "        optimizer = self.optimizer_class(self.model.parameters(), lr=min_lr)\n",
    "        \n",
    "        # Calculate LR step size for linear increase\n",
    "        lr_lambda = lambda batch_num: min_lr + (max_lr - min_lr) * batch_num / (num_batches - 1)\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(enumerate(train_loader), total=num_batches, desc=\"Linear LR Test\")\n",
    "        \n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            # Move data to device\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Calculate current learning rate\n",
    "            current_lr = lr_lambda(batch_idx)\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get gradient norm before optimizer step\n",
    "            grad_norm = self._get_gradient_norm()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store results\n",
    "            current_loss = loss.item()\n",
    "            self.results['learning_rates'].append(current_lr)\n",
    "            self.results['losses'].append(current_loss)\n",
    "            self.results['gradients'].append(grad_norm)\n",
    "            \n",
    "            # Calculate smooth loss\n",
    "            if len(self.results['smooth_losses']) == 0:\n",
    "                smooth_loss = current_loss\n",
    "            else:\n",
    "                smooth_loss = (smooth_factor * self.results['smooth_losses'][-1] + \n",
    "                             (1 - smooth_factor) * current_loss)\n",
    "            self.results['smooth_losses'].append(smooth_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'LR': f'{current_lr:.2e}',\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Smooth': f'{smooth_loss:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping check\n",
    "            if self._should_stop_early(current_loss, threshold=stop_div_factor):\n",
    "                print(f\"\\\\nüõë Early stopping at batch {batch_idx} (loss exploded)\")\n",
    "                break\n",
    "            \n",
    "            # Update best loss\n",
    "            if smooth_loss < self.best_loss:\n",
    "                self.best_loss = smooth_loss\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Find optimal learning rate\n",
    "        self.results['optimal_lr'], self.results['optimal_lr_reason'] = self._find_optimal_lr_simple()\n",
    "        \n",
    "        # Store experiment info\n",
    "        self.results['experiment_info'] = {\n",
    "            'min_lr': min_lr,\n",
    "            'max_lr': max_lr,\n",
    "            'num_batches_tested': len(self.results['learning_rates']),\n",
    "            'stop_div_factor': stop_div_factor,\n",
    "            'smooth_factor': smooth_factor\n",
    "        }\n",
    "        \n",
    "        # Restore model state\n",
    "        self._restore_model()\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Linear LR Range Test completed!\")\n",
    "        print(f\"üéØ Optimal LR: {self.results['optimal_lr']:.2e}\")\n",
    "        print(f\"üìù Reason: {self.results['optimal_lr_reason']}\")\n",
    "        \n",
    "        return self.results.copy()\n",
    "    \n",
    "    def plot_results(self, save_plot=True, show_plot=True):\n",
    "        \"\"\"Plot the results of linear LR range test\"\"\"\n",
    "        if not self.results['learning_rates']:\n",
    "            print(\"‚ùå No results to plot. Run find_lr() first.\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Loss vs Learning Rate (log scale)\n",
    "        ax1.semilogx(self.results['learning_rates'], self.results['losses'], \n",
    "                     'b-', alpha=0.6, label='Raw Loss')\n",
    "        ax1.semilogx(self.results['learning_rates'], self.results['smooth_losses'], \n",
    "                     'r-', linewidth=2, label='Smoothed Loss')\n",
    "        \n",
    "        # Mark optimal LR\n",
    "        if self.results['optimal_lr']:\n",
    "            ax1.axvline(x=self.results['optimal_lr'], color='green', linestyle='--', \n",
    "                       linewidth=2, label=f'Optimal LR: {self.results[\"optimal_lr\"]:.2e}')\n",
    "        \n",
    "        ax1.set_xlabel('Learning Rate')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Linear LR Range Test: Loss vs Learning Rate')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Gradient Norm vs Learning Rate\n",
    "        ax2.semilogx(self.results['learning_rates'], self.results['gradients'], \n",
    "                     'purple', alpha=0.7, label='Gradient Norm')\n",
    "        ax2.set_xlabel('Learning Rate')\n",
    "        ax2.set_ylabel('Gradient Norm')\n",
    "        ax2.set_title('Gradient Norm vs Learning Rate')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plot:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            plot_path = BASE_DIR / 'plots' / f'linear_lr_test_{timestamp}.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Plot saved to: {plot_path}\")\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Test the Linear LR Finder\n",
    "print(\"‚úÖ Linear LR Range Test implemented successfully!\")\n",
    "print(\"üîß Features: Linear LR increase, early stopping, gradient tracking\")\n",
    "print(\"üìä Use: finder.find_lr(train_loader, min_lr=1e-7, max_lr=10.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a3fa4",
   "metadata": {},
   "source": [
    "## 5. Implement Exponential LR Range Test\n",
    "\n",
    "The **Exponential LR Range Test** is faster and more aggressive than the linear version. It works by:\n",
    "\n",
    "1. **Exponential growth**: LR increases exponentially (multiplicative steps)\n",
    "2. **Faster exploration**: Covers wide LR ranges quickly\n",
    "3. **Better for large ranges**: When you don't know the approximate optimal LR\n",
    "4. **Aggressive search**: Finds upper bounds more effectively\n",
    "\n",
    "### When to use:\n",
    "- ‚úÖ **Unknown LR range** - when you have no idea about optimal LR\n",
    "- ‚úÖ **Quick exploration** - fast initial assessment\n",
    "- ‚úÖ **Large models** - when linear search takes too long\n",
    "- ‚úÖ **Comparative studies** - comparing with linear method\n",
    "\n",
    "### Theory:\n",
    "- **Exponential formula**: `LR(t) = min_lr * (max_lr/min_lr)^(t/total_steps)`\n",
    "- **Faster convergence**: Reaches high LR values quickly\n",
    "- **Logarithmic scale**: Natural for visualizing wide LR ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f71595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialLRFinder(BaseLRFinder):\n",
    "    \"\"\"\n",
    "    Exponential Learning Rate Range Test\n",
    "    \n",
    "    Increases learning rate exponentially from min_lr to max_lr\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer_class=optim.Adam, device=None):\n",
    "        super().__init__(model, criterion, optimizer_class, device)\n",
    "        self.results['method'] = 'Exponential_LR_Range_Test'\n",
    "    \n",
    "    def find_lr(self, train_loader, min_lr=1e-7, max_lr=10.0, num_batches=None,\n",
    "                stop_div_factor=4.0, smooth_factor=0.98):\n",
    "        \"\"\"\n",
    "        Find optimal learning rate using exponential range test\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            min_lr: Minimum learning rate to test\n",
    "            max_lr: Maximum learning rate to test\n",
    "            num_batches: Number of batches to test (default: full epoch)\n",
    "            stop_div_factor: Stop if loss > stop_div_factor * best_loss\n",
    "            smooth_factor: Smoothing factor for loss\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results with optimal LR and loss curves\n",
    "        \"\"\"\n",
    "        print(f\"üöÄ Starting Exponential LR Range Test...\")\n",
    "        print(f\"üìä LR Range: {min_lr:.2e} ‚Üí {max_lr:.2e} (exponential)\")\n",
    "        \n",
    "        # Prepare model and reset results\n",
    "        self._prepare_model()\n",
    "        self.results = {key: [] for key in ['learning_rates', 'losses', 'gradients', 'smooth_losses']}\n",
    "        self.results['method'] = 'Exponential_LR_Range_Test'\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Determine number of batches\n",
    "        if num_batches is None:\n",
    "            num_batches = len(train_loader)\n",
    "        num_batches = min(num_batches, len(train_loader))\n",
    "        \n",
    "        print(f\"üéØ Testing over {num_batches} batches\")\n",
    "        \n",
    "        # Create optimizer with minimum LR\n",
    "        optimizer = self.optimizer_class(self.model.parameters(), lr=min_lr)\n",
    "        \n",
    "        # Calculate exponential multiplier\n",
    "        # LR(t) = min_lr * (max_lr/min_lr)^(t/(num_batches-1))\n",
    "        lr_multiplier = (max_lr / min_lr) ** (1.0 / (num_batches - 1))\n",
    "        \n",
    "        print(f\"üìà LR multiplier per batch: {lr_multiplier:.6f}\")\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(enumerate(train_loader), total=num_batches, desc=\"Exponential LR Test\")\n",
    "        \n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            # Move data to device\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Calculate current learning rate (exponential)\n",
    "            current_lr = min_lr * (lr_multiplier ** batch_idx)\n",
    "            \n",
    "            # Ensure we don't exceed max_lr due to floating point errors\n",
    "            current_lr = min(current_lr, max_lr)\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get gradient norm before optimizer step\n",
    "            grad_norm = self._get_gradient_norm()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store results\n",
    "            current_loss = loss.item()\n",
    "            self.results['learning_rates'].append(current_lr)\n",
    "            self.results['losses'].append(current_loss)\n",
    "            self.results['gradients'].append(grad_norm)\n",
    "            \n",
    "            # Calculate smooth loss\n",
    "            if len(self.results['smooth_losses']) == 0:\n",
    "                smooth_loss = current_loss\n",
    "            else:\n",
    "                smooth_loss = (smooth_factor * self.results['smooth_losses'][-1] + \n",
    "                             (1 - smooth_factor) * current_loss)\n",
    "            self.results['smooth_losses'].append(smooth_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'LR': f'{current_lr:.2e}',\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Smooth': f'{smooth_loss:.4f}',\n",
    "                'Mult': f'{lr_multiplier:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping check\n",
    "            if self._should_stop_early(current_loss, threshold=stop_div_factor):\n",
    "                print(f\"\\\\nüõë Early stopping at batch {batch_idx} (loss exploded)\")\n",
    "                break\n",
    "            \n",
    "            # Update best loss\n",
    "            if smooth_loss < self.best_loss:\n",
    "                self.best_loss = smooth_loss\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Find optimal learning rate\n",
    "        self.results['optimal_lr'], self.results['optimal_lr_reason'] = self._find_optimal_lr_exponential()\n",
    "        \n",
    "        # Store experiment info\n",
    "        self.results['experiment_info'] = {\n",
    "            'min_lr': min_lr,\n",
    "            'max_lr': max_lr,\n",
    "            'num_batches_tested': len(self.results['learning_rates']),\n",
    "            'lr_multiplier': lr_multiplier,\n",
    "            'stop_div_factor': stop_div_factor,\n",
    "            'smooth_factor': smooth_factor\n",
    "        }\n",
    "        \n",
    "        # Restore model state\n",
    "        self._restore_model()\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Exponential LR Range Test completed!\")\n",
    "        print(f\"üéØ Optimal LR: {self.results['optimal_lr']:.2e}\")\n",
    "        print(f\"üìù Reason: {self.results['optimal_lr_reason']}\")\n",
    "        \n",
    "        return self.results.copy()\n",
    "    \n",
    "    def _find_optimal_lr_exponential(self):\n",
    "        \"\"\"\n",
    "        Find optimal LR for exponential test - accounts for logarithmic scale\n",
    "        \"\"\"\n",
    "        if len(self.results['smooth_losses']) < 10:\n",
    "            return self.results['learning_rates'][len(self.results['losses'])//2], \"Middle of range (insufficient data)\"\n",
    "        \n",
    "        smooth_losses = np.array(self.results['smooth_losses'])\n",
    "        learning_rates = np.array(self.results['learning_rates'])\n",
    "        \n",
    "        # Convert to log scale for exponential analysis\n",
    "        log_lrs = np.log10(learning_rates)\n",
    "        \n",
    "        # Find steepest descent in log space\n",
    "        log_gradients = np.gradient(smooth_losses, log_lrs)\n",
    "        min_gradient_idx = np.argmin(log_gradients)\n",
    "        \n",
    "        # Find minimum loss\n",
    "        min_loss_idx = np.argmin(smooth_losses)\n",
    "        \n",
    "        # For exponential, be more aggressive (use steepest descent)\n",
    "        optimal_idx = min_gradient_idx\n",
    "        \n",
    "        # Safety bounds\n",
    "        optimal_idx = max(2, min(optimal_idx, len(learning_rates) - 3))\n",
    "        \n",
    "        optimal_lr = learning_rates[optimal_idx]\n",
    "        reason = f\"Steepest descent in log-space at step {optimal_idx} (loss: {smooth_losses[optimal_idx]:.4f})\"\n",
    "        \n",
    "        return optimal_lr, reason\n",
    "    \n",
    "    def plot_results(self, save_plot=True, show_plot=True):\n",
    "        \"\"\"Plot the results of exponential LR range test\"\"\"\n",
    "        if not self.results['learning_rates']:\n",
    "            print(\"‚ùå No results to plot. Run find_lr() first.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Loss vs Learning Rate (log scale)\n",
    "        ax1.semilogx(self.results['learning_rates'], self.results['losses'], \n",
    "                     'b-', alpha=0.6, label='Raw Loss')\n",
    "        ax1.semilogx(self.results['learning_rates'], self.results['smooth_losses'], \n",
    "                     'r-', linewidth=2, label='Smoothed Loss')\n",
    "        \n",
    "        if self.results['optimal_lr']:\n",
    "            ax1.axvline(x=self.results['optimal_lr'], color='green', linestyle='--', \n",
    "                       linewidth=2, label=f'Optimal LR: {self.results[\"optimal_lr\"]:.2e}')\n",
    "        \n",
    "        ax1.set_xlabel('Learning Rate')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Exponential LR Test: Loss vs Learning Rate')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Loss vs Batch (time series)\n",
    "        batches = range(len(self.results['losses']))\n",
    "        ax2.plot(batches, self.results['losses'], 'b-', alpha=0.6, label='Raw Loss')\n",
    "        ax2.plot(batches, self.results['smooth_losses'], 'r-', linewidth=2, label='Smoothed Loss')\n",
    "        ax2.set_xlabel('Batch Number')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Loss vs Training Steps')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Gradient Norm vs Learning Rate\n",
    "        ax3.semilogx(self.results['learning_rates'], self.results['gradients'], \n",
    "                     'purple', alpha=0.7, label='Gradient Norm')\n",
    "        ax3.set_xlabel('Learning Rate')\n",
    "        ax3.set_ylabel('Gradient Norm') \n",
    "        ax3.set_title('Gradient Norm vs Learning Rate')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Learning Rate Schedule\n",
    "        ax4.semilogy(batches, self.results['learning_rates'], 'orange', linewidth=2, \n",
    "                    label='LR Schedule (Exponential)')\n",
    "        ax4.set_xlabel('Batch Number')\n",
    "        ax4.set_ylabel('Learning Rate')\n",
    "        ax4.set_title('Exponential Learning Rate Schedule')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plot:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            plot_path = BASE_DIR / 'plots' / f'exponential_lr_test_{timestamp}.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Plot saved to: {plot_path}\")\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "print(\"‚úÖ Exponential LR Range Test implemented successfully!\")\n",
    "print(\"üöÄ Features: Exponential LR growth, log-scale analysis, aggressive search\")\n",
    "print(\"üìä Use: finder.find_lr(train_loader, min_lr=1e-7, max_lr=10.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a718c37",
   "metadata": {},
   "source": [
    "## 6. Implement Cyclical LR Range Test\n",
    "\n",
    "The **Cyclical LR Range Test** uses cyclical patterns to explore learning rate ranges. It's based on the idea that:\n",
    "\n",
    "1. **Cyclical patterns**: LR oscillates between min and max values\n",
    "2. **Multiple explorations**: Each cycle explores the LR range differently\n",
    "3. **Pattern analysis**: Different patterns (triangular, cosine) reveal different insights\n",
    "4. **Robust testing**: Less sensitive to single bad batches\n",
    "\n",
    "### When to use:\n",
    "- ‚úÖ **Noisy datasets** - when single LR sweeps are unreliable\n",
    "- ‚úÖ **Robust estimation** - want multiple confirmations of optimal LR\n",
    "- ‚úÖ **Cyclical training plans** - planning to use cyclical LR schedules\n",
    "- ‚úÖ **Research purposes** - comparing different cyclical patterns\n",
    "\n",
    "### Patterns Available:\n",
    "1. **Triangular**: Linear up, linear down (classic cyclical LR)\n",
    "2. **Cosine**: Smooth cosine annealing pattern\n",
    "3. **Exponential**: Exponential up, exponential down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87967523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicalLRFinder(BaseLRFinder):\n",
    "    \"\"\"\n",
    "    Cyclical Learning Rate Range Test\n",
    "    \n",
    "    Uses cyclical patterns to explore learning rate ranges\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer_class=optim.Adam, device=None):\n",
    "        super().__init__(model, criterion, optimizer_class, device)\n",
    "        self.results['method'] = 'Cyclical_LR_Range_Test'\n",
    "    \n",
    "    def find_lr(self, train_loader, min_lr=1e-7, max_lr=1.0, num_cycles=2, \n",
    "                cycle_pattern='triangular', num_batches=None, stop_div_factor=4.0, \n",
    "                smooth_factor=0.98):\n",
    "        \"\"\"\n",
    "        Find optimal learning rate using cyclical range test\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            min_lr: Minimum learning rate in cycles\n",
    "            max_lr: Maximum learning rate in cycles\n",
    "            num_cycles: Number of complete cycles to perform\n",
    "            cycle_pattern: 'triangular', 'cosine', or 'exponential'\n",
    "            num_batches: Number of batches to test (default: full epoch * num_cycles)\n",
    "            stop_div_factor: Stop if loss > stop_div_factor * best_loss\n",
    "            smooth_factor: Smoothing factor for loss\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results with optimal LR and loss curves\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Starting Cyclical LR Range Test...\")\n",
    "        print(f\"üìä LR Range: {min_lr:.2e} ‚Üî {max_lr:.2e}\")\n",
    "        print(f\"üéØ Pattern: {cycle_pattern.title()}, Cycles: {num_cycles}\")\n",
    "        \n",
    "        # Prepare model and reset results\n",
    "        self._prepare_model()\n",
    "        self.results = {key: [] for key in ['learning_rates', 'losses', 'gradients', 'smooth_losses']}\n",
    "        self.results['method'] = 'Cyclical_LR_Range_Test'\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Determine number of batches\n",
    "        if num_batches is None:\n",
    "            num_batches = len(train_loader) * num_cycles\n",
    "        num_batches = min(num_batches, len(train_loader) * num_cycles)\n",
    "        \n",
    "        print(f\"üéØ Testing over {num_batches} batches ({num_batches//num_cycles} per cycle)\")\n",
    "        \n",
    "        # Create optimizer with minimum LR\n",
    "        optimizer = self.optimizer_class(self.model.parameters(), lr=min_lr)\n",
    "        \n",
    "        # Define cyclical LR functions\n",
    "        def triangular_lr(batch_num, cycle_length):\n",
    "            cycle_pos = (batch_num % cycle_length) / cycle_length\n",
    "            if cycle_pos <= 0.5:\n",
    "                # First half: increase linearly\n",
    "                return min_lr + (max_lr - min_lr) * (cycle_pos * 2)\n",
    "            else:\n",
    "                # Second half: decrease linearly\n",
    "                return max_lr - (max_lr - min_lr) * ((cycle_pos - 0.5) * 2)\n",
    "        \n",
    "        def cosine_lr(batch_num, cycle_length):\n",
    "            cycle_pos = (batch_num % cycle_length) / cycle_length\n",
    "            return min_lr + (max_lr - min_lr) * (1 + np.cos(np.pi * cycle_pos)) / 2\n",
    "        \n",
    "        def exponential_lr(batch_num, cycle_length):\n",
    "            cycle_pos = (batch_num % cycle_length) / cycle_length\n",
    "            if cycle_pos <= 0.5:\n",
    "                # First half: exponential increase\n",
    "                t = cycle_pos * 2\n",
    "                return min_lr * ((max_lr / min_lr) ** t)\n",
    "            else:\n",
    "                # Second half: exponential decrease\n",
    "                t = 1 - (cycle_pos - 0.5) * 2\n",
    "                return min_lr * ((max_lr / min_lr) ** t)\n",
    "        \n",
    "        # Select pattern function\n",
    "        pattern_funcs = {\n",
    "            'triangular': triangular_lr,\n",
    "            'cosine': cosine_lr,\n",
    "            'exponential': exponential_lr\n",
    "        }\n",
    "        \n",
    "        if cycle_pattern not in pattern_funcs:\n",
    "            print(f\"‚ö†Ô∏è Unknown pattern '{cycle_pattern}', using 'triangular'\")\n",
    "            cycle_pattern = 'triangular'\n",
    "        \n",
    "        lr_func = pattern_funcs[cycle_pattern]\n",
    "        cycle_length = num_batches // num_cycles\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(range(num_batches), desc=f\"Cyclical LR Test ({cycle_pattern})\")\n",
    "        \n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        for batch_idx in pbar:\n",
    "            try:\n",
    "                data, target = next(data_iter)\n",
    "            except StopIteration:\n",
    "                # Reset iterator when we reach the end\n",
    "                data_iter = iter(train_loader)\n",
    "                data, target = next(data_iter)\n",
    "            \n",
    "            # Move data to device\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Calculate current learning rate using cyclical pattern\n",
    "            current_lr = lr_func(batch_idx, cycle_length)\n",
    "            \n",
    "            # Update optimizer learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get gradient norm\n",
    "            grad_norm = self._get_gradient_norm()\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store results\n",
    "            current_loss = loss.item()\n",
    "            self.results['learning_rates'].append(current_lr)\n",
    "            self.results['losses'].append(current_loss)\n",
    "            self.results['gradients'].append(grad_norm)\n",
    "            \n",
    "            # Calculate smooth loss\n",
    "            if len(self.results['smooth_losses']) == 0:\n",
    "                smooth_loss = current_loss\n",
    "            else:\n",
    "                smooth_loss = (smooth_factor * self.results['smooth_losses'][-1] + \n",
    "                             (1 - smooth_factor) * current_loss)\n",
    "            self.results['smooth_losses'].append(smooth_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            cycle_num = batch_idx // cycle_length + 1\n",
    "            cycle_pos = (batch_idx % cycle_length) / cycle_length\n",
    "            pbar.set_postfix({\n",
    "                'Cycle': f'{cycle_num}/{num_cycles}',\n",
    "                'Pos': f'{cycle_pos:.2f}',\n",
    "                'LR': f'{current_lr:.2e}',\n",
    "                'Loss': f'{current_loss:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping check (but be more lenient for cyclical)\n",
    "            if self._should_stop_early(current_loss, threshold=stop_div_factor * 2):\n",
    "                print(f\"\\\\nüõë Early stopping at batch {batch_idx} (loss exploded)\")\n",
    "                break\n",
    "            \n",
    "            # Update best loss\n",
    "            if smooth_loss < self.best_loss:\n",
    "                self.best_loss = smooth_loss\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Find optimal learning rate using cyclical-specific method\n",
    "        self.results['optimal_lr'], self.results['optimal_lr_reason'] = self._find_optimal_lr_cyclical()\n",
    "        \n",
    "        # Store experiment info\n",
    "        self.results['experiment_info'] = {\n",
    "            'min_lr': min_lr,\n",
    "            'max_lr': max_lr,\n",
    "            'num_cycles': num_cycles,\n",
    "            'cycle_pattern': cycle_pattern,\n",
    "            'cycle_length': cycle_length,\n",
    "            'num_batches_tested': len(self.results['learning_rates']),\n",
    "            'stop_div_factor': stop_div_factor,\n",
    "            'smooth_factor': smooth_factor\n",
    "        }\n",
    "        \n",
    "        # Restore model state\n",
    "        self._restore_model()\n",
    "        \n",
    "        print(f\"\\\\n‚úÖ Cyclical LR Range Test completed!\")\n",
    "        print(f\"üéØ Optimal LR: {self.results['optimal_lr']:.2e}\")\n",
    "        print(f\"üìù Reason: {self.results['optimal_lr_reason']}\")\n",
    "        \n",
    "        return self.results.copy()\n",
    "    \n",
    "    def _find_optimal_lr_cyclical(self):\n",
    "        \"\"\"Find optimal LR for cyclical test - considers all cycles\"\"\"\n",
    "        if len(self.results['smooth_losses']) < 20:\n",
    "            return self.results['learning_rates'][len(self.results['losses'])//2], \"Middle of range (insufficient data)\"\n",
    "        \n",
    "        smooth_losses = np.array(self.results['smooth_losses'])\n",
    "        learning_rates = np.array(self.results['learning_rates'])\n",
    "        \n",
    "        # Group by LR ranges to find consistent performers\n",
    "        lr_loss_pairs = list(zip(learning_rates, smooth_losses))\n",
    "        \n",
    "        # Find the LR that consistently gives good performance\n",
    "        lr_bins = np.logspace(np.log10(min(learning_rates)), np.log10(max(learning_rates)), 50)\n",
    "        bin_losses = defaultdict(list)\n",
    "        \n",
    "        for lr, loss in lr_loss_pairs:\n",
    "            # Find which bin this LR belongs to\n",
    "            bin_idx = np.digitize(lr, lr_bins) - 1\n",
    "            bin_idx = max(0, min(bin_idx, len(lr_bins) - 1))\n",
    "            bin_losses[bin_idx].append(loss)\n",
    "        \n",
    "        # Calculate average loss for each bin\n",
    "        bin_avg_losses = {}\n",
    "        for bin_idx, losses in bin_losses.items():\n",
    "            if len(losses) >= 2:  # Need at least 2 samples\n",
    "                bin_avg_losses[bin_idx] = np.mean(losses)\n",
    "        \n",
    "        if not bin_avg_losses:\n",
    "            # Fallback to simple method\n",
    "            return self._find_optimal_lr_simple()\n",
    "        \n",
    "        # Find bin with minimum average loss\n",
    "        best_bin_idx = min(bin_avg_losses.keys(), key=lambda x: bin_avg_losses[x])\n",
    "        optimal_lr = lr_bins[best_bin_idx]\n",
    "        \n",
    "        reason = f\"Consistently good performance across cycles (avg loss: {bin_avg_losses[best_bin_idx]:.4f})\"\n",
    "        \n",
    "        return optimal_lr, reason\n",
    "    \n",
    "    def plot_results(self, save_plot=True, show_plot=True):\n",
    "        \"\"\"Plot the results of cyclical LR range test\"\"\"\n",
    "        if not self.results['learning_rates']:\n",
    "            print(\"‚ùå No results to plot. Run find_lr() first.\")\n",
    "            return\n",
    "        \n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = GridSpec(3, 3, figure=fig)\n",
    "        \n",
    "        # Main plot: Loss vs Learning Rate (cyclical)\n",
    "        ax1 = fig.add_subplot(gs[0, :2])\n",
    "        \n",
    "        # Color-code by cycle if we have cycle info\n",
    "        if 'cycle_length' in self.results.get('experiment_info', {}):\n",
    "            cycle_length = self.results['experiment_info']['cycle_length']\n",
    "            num_cycles = self.results['experiment_info']['num_cycles']\n",
    "            \n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, num_cycles))\n",
    "            \n",
    "            for cycle in range(num_cycles):\n",
    "                start_idx = cycle * cycle_length\n",
    "                end_idx = min(start_idx + cycle_length, len(self.results['learning_rates']))\n",
    "                \n",
    "                if start_idx < len(self.results['learning_rates']):\n",
    "                    cycle_lrs = self.results['learning_rates'][start_idx:end_idx]\n",
    "                    cycle_losses = self.results['smooth_losses'][start_idx:end_idx]\n",
    "                    \n",
    "                    ax1.semilogx(cycle_lrs, cycle_losses, color=colors[cycle], \n",
    "                               linewidth=2, label=f'Cycle {cycle+1}', alpha=0.8)\n",
    "        else:\n",
    "            ax1.semilogx(self.results['learning_rates'], self.results['smooth_losses'], \n",
    "                        'b-', linewidth=2, label='All Cycles')\n",
    "        \n",
    "        if self.results['optimal_lr']:\n",
    "            ax1.axvline(x=self.results['optimal_lr'], color='green', linestyle='--', \n",
    "                       linewidth=3, label=f'Optimal LR: {self.results[\"optimal_lr\"]:.2e}')\n",
    "        \n",
    "        ax1.set_xlabel('Learning Rate')\n",
    "        ax1.set_ylabel('Smoothed Loss')\n",
    "        ax1.set_title(f'Cyclical LR Test: {self.results.get(\"experiment_info\", {}).get(\"cycle_pattern\", \"Unknown\").title()} Pattern')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series plot\n",
    "        ax2 = fig.add_subplot(gs[1, :2])\n",
    "        batches = range(len(self.results['losses']))\n",
    "        ax2.plot(batches, self.results['losses'], 'lightblue', alpha=0.6, label='Raw Loss')\n",
    "        ax2.plot(batches, self.results['smooth_losses'], 'red', linewidth=2, label='Smoothed Loss')\n",
    "        ax2.set_xlabel('Batch Number')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Loss vs Training Steps')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        ax3 = fig.add_subplot(gs[2, :2])\n",
    "        ax3.semilogy(batches, self.results['learning_rates'], 'orange', linewidth=2, \n",
    "                    label=f'{self.results.get(\"experiment_info\", {}).get(\"cycle_pattern\", \"Unknown\").title()} LR Schedule')\n",
    "        ax3.set_xlabel('Batch Number')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_title('Cyclical Learning Rate Schedule')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norms\n",
    "        ax4 = fig.add_subplot(gs[0, 2])\n",
    "        ax4.semilogx(self.results['learning_rates'], self.results['gradients'], \n",
    "                    'purple', alpha=0.7, label='Gradient Norm')\n",
    "        ax4.set_xlabel('Learning Rate')\n",
    "        ax4.set_ylabel('Gradient Norm')\n",
    "        ax4.set_title('Gradient vs LR')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss distribution by LR range\n",
    "        ax5 = fig.add_subplot(gs[1:, 2])\n",
    "        \n",
    "        # Create LR bins and plot loss distribution\n",
    "        lr_array = np.array(self.results['learning_rates'])\n",
    "        loss_array = np.array(self.results['smooth_losses'])\n",
    "        \n",
    "        # Create scatter plot with color mapping\n",
    "        scatter = ax5.scatter(lr_array, loss_array, c=range(len(lr_array)), \n",
    "                            cmap='viridis', alpha=0.6, s=10)\n",
    "        ax5.set_xscale('log')\n",
    "        ax5.set_xlabel('Learning Rate')\n",
    "        ax5.set_ylabel('Smoothed Loss')\n",
    "        ax5.set_title('Loss Distribution\\\\n(Color = Time)')\n",
    "        plt.colorbar(scatter, ax=ax5, label='Batch Number')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plot:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            plot_path = BASE_DIR / 'plots' / f'cyclical_lr_test_{timestamp}.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Plot saved to: {plot_path}\")\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "print(\"‚úÖ Cyclical LR Range Test implemented successfully!\")\n",
    "print(\"üîÑ Features: Multiple cycles, pattern options (triangular/cosine/exponential)\")\n",
    "print(\"üìä Use: finder.find_lr(train_loader, min_lr=1e-5, max_lr=1.0, num_cycles=2, cycle_pattern='triangular')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcad1b",
   "metadata": {},
   "source": [
    "## 7. Create Sample Dataset and Model\n",
    "\n",
    "Let's create a sample dataset and model to demonstrate our LR finder toolkit. We'll use:\n",
    "\n",
    "1. **CIFAR-10 style data**: Small images with 10 classes\n",
    "2. **Flexible model**: CNN that works well for demonstrations\n",
    "3. **Synthetic option**: Generate data if CIFAR-10 isn't available\n",
    "4. **Any model support**: Show how to use with different architectures\n",
    "\n",
    "### Model Features:\n",
    "- üèóÔ∏è **Configurable**: Easy to modify architecture\n",
    "- üéØ **Realistic**: Represents real-world training scenarios  \n",
    "- üîß **Lightweight**: Fast enough for demonstrations\n",
    "- üìä **Informative**: Shows clear LR sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de035d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample CNN Model for demonstration\n",
    "class SampleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Sample CNN model for demonstrating LR finder\n",
    "    - Works with any input size (default: 32x32)\n",
    "    - Configurable depth and width\n",
    "    - Good for LR sensitivity analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, input_channels=3, base_channels=32):\n",
    "        super(SampleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, base_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(base_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(base_channels, base_channels*2, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(base_channels*2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(base_channels*2, base_channels*4, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(base_channels*4)\n",
    "        \n",
    "        # Adaptive pooling to handle any input size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # Classifier\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(base_channels*4 * 16, num_classes)  # 4*4 = 16\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Conv block 2  \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Adaptive pooling and classifier\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_synthetic_dataset(num_samples=1000, image_size=32, num_classes=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset for demonstration\n",
    "    \"\"\"\n",
    "    print(f\"üé® Creating synthetic dataset...\")\n",
    "    print(f\"   üìä Samples: {num_samples}, Classes: {num_classes}\")\n",
    "    print(f\"   üñºÔ∏è Image size: {image_size}x{image_size}\")\n",
    "    \n",
    "    # Generate random images with some structure\n",
    "    images = torch.randn(num_samples, 3, image_size, image_size)\n",
    "    \n",
    "    # Add some patterns to make it learnable\n",
    "    for i in range(num_samples):\n",
    "        class_id = i % num_classes\n",
    "        # Add class-specific patterns\n",
    "        if class_id % 2 == 0:\n",
    "            images[i, 0, :5, :5] = 1.0  # Red corner for even classes\n",
    "        if class_id % 3 == 0:\n",
    "            images[i, 1, -5:, -5:] = 1.0  # Green corner for multiples of 3\n",
    "        if class_id % 5 == 0:\n",
    "            images[i, 2, 10:20, 10:20] = 1.0  # Blue center for multiples of 5\n",
    "    \n",
    "    # Generate labels\n",
    "    labels = torch.tensor([i % num_classes for i in range(num_samples)])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(images, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"‚úÖ Synthetic dataset created: {len(dataset)} samples, {len(dataloader)} batches\")\n",
    "    return dataloader, dataset\n",
    "\n",
    "def create_cifar10_dataset(batch_size=32, subset_size=None):\n",
    "    \"\"\"\n",
    "    Create CIFAR-10 dataset (falls back to synthetic if not available)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torchvision\n",
    "        import torchvision.transforms as transforms\n",
    "        \n",
    "        print(\"üñºÔ∏è Loading CIFAR-10 dataset...\")\n",
    "        \n",
    "        # Simple transforms for demonstration\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        # Load CIFAR-10\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=True, download=True, transform=transform\n",
    "        )\n",
    "        \n",
    "        # Use subset if specified\n",
    "        if subset_size and subset_size < len(trainset):\n",
    "            indices = torch.randperm(len(trainset))[:subset_size]\n",
    "            trainset = torch.utils.data.Subset(trainset, indices)\n",
    "            print(f\"üìä Using subset: {subset_size} samples\")\n",
    "        \n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        print(f\"‚úÖ CIFAR-10 loaded: {len(trainset)} samples, {len(trainloader)} batches\")\n",
    "        return trainloader, trainset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load CIFAR-10: {e}\")\n",
    "        print(\"üé® Falling back to synthetic dataset...\")\n",
    "        return create_synthetic_dataset(num_samples=1000, batch_size=batch_size)\n",
    "\n",
    "def create_sample_models():\n",
    "    \"\"\"\n",
    "    Create various sample models to demonstrate LR finder flexibility\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # 1. Small CNN\n",
    "    models['small_cnn'] = SampleCNN(num_classes=10, base_channels=16)\n",
    "    \n",
    "    # 2. Medium CNN\n",
    "    models['medium_cnn'] = SampleCNN(num_classes=10, base_channels=32)\n",
    "    \n",
    "    # 3. Large CNN\n",
    "    models['large_cnn'] = SampleCNN(num_classes=10, base_channels=64)\n",
    "    \n",
    "    # 4. Simple MLP (for comparison)\n",
    "    class SimpleMLP(nn.Module):\n",
    "        def __init__(self, input_size=3*32*32, hidden_size=512, num_classes=10):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "            self.fc3 = nn.Linear(hidden_size//2, num_classes)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    models['simple_mlp'] = SimpleMLP()\n",
    "    \n",
    "    # Print model info\n",
    "    print(\"üèóÔ∏è Sample models created:\")\n",
    "    for name, model in models.items():\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"   {name}: {num_params:,} parameters\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Create dataset and models\n",
    "print(\"üöÄ Setting up demonstration environment...\")\n",
    "\n",
    "# Create dataset (try CIFAR-10, fallback to synthetic)\n",
    "train_loader, train_dataset = create_cifar10_dataset(batch_size=64, subset_size=2000)\n",
    "\n",
    "# Create sample models\n",
    "sample_models = create_sample_models()\n",
    "\n",
    "# Choose a default model for demonstrations\n",
    "demo_model = sample_models['medium_cnn'].to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\\\\n‚úÖ Demo setup complete!\")\n",
    "print(f\"üìä Dataset: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"üèóÔ∏è Demo model: {demo_model.__class__.__name__} with {sum(p.numel() for p in demo_model.parameters()):,} parameters\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üìã Loss function: {criterion.__class__.__name__}\")\n",
    "\n",
    "# Quick test to ensure everything works\n",
    "print(\"\\\\nüß™ Quick functionality test...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "test_input, test_target = sample_batch[0][:4].to(device), sample_batch[1][:4].to(device)\n",
    "with torch.no_grad():\n",
    "    test_output = demo_model(test_input)\n",
    "    test_loss = criterion(test_output, test_target)\n",
    "print(f\"‚úÖ Test passed - Loss: {test_loss.item():.4f}\")\n",
    "print(f\"üîß Ready for LR finding experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04778a1a",
   "metadata": {},
   "source": [
    "## 8. Execute Different LR Finding Methods\n",
    "\n",
    "Now let's run all our LR finding methods on the sample model and collect results for comparison. We'll execute:\n",
    "\n",
    "1. **Linear LR Range Test** - Conservative, thorough exploration\n",
    "2. **Exponential LR Range Test** - Fast, aggressive exploration  \n",
    "3. **Cyclical LR Range Test** - Robust, pattern-based exploration\n",
    "\n",
    "### Execution Strategy:\n",
    "- üéØ **Same conditions**: All methods use the same model state and data\n",
    "- üìä **Consistent parameters**: Similar LR ranges and batch counts\n",
    "- üíæ **Full tracking**: Save all results for detailed comparison\n",
    "- üîÑ **Model reset**: Fresh start for each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all LR finding methods\n",
    "print(\"üöÄ Starting comprehensive LR finding experiment...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define common parameters for fair comparison\n",
    "common_params = {\n",
    "    'min_lr': 1e-6,\n",
    "    'max_lr': 1.0,\n",
    "    'num_batches': 100,  # Limit for demonstration\n",
    "    'stop_div_factor': 4.0,\n",
    "    'smooth_factor': 0.98\n",
    "}\n",
    "\n",
    "print(f\"üìä Common parameters:\")\n",
    "for key, value in common_params.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Storage for all results\n",
    "all_results = {}\n",
    "all_optimal_lrs = {}\n",
    "\n",
    "# Method 1: Linear LR Range Test\n",
    "print(\"üîç Method 1: Linear LR Range Test\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh model for this method\n",
    "linear_model = SampleCNN(num_classes=10, base_channels=32).to(device)\n",
    "linear_finder = LinearLRFinder(linear_model, criterion, optim.Adam, device)\n",
    "\n",
    "# Run linear test\n",
    "linear_results = linear_finder.find_lr(\n",
    "    train_loader,\n",
    "    min_lr=common_params['min_lr'],\n",
    "    max_lr=common_params['max_lr'],\n",
    "    num_batches=common_params['num_batches'],\n",
    "    stop_div_factor=common_params['stop_div_factor'],\n",
    "    smooth_factor=common_params['smooth_factor']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "linear_finder.save_results(\"method_comparison_linear\")\n",
    "all_results['Linear'] = linear_results\n",
    "all_optimal_lrs['Linear'] = linear_results['optimal_lr']\n",
    "\n",
    "print(f\"‚úÖ Linear method completed. Optimal LR: {linear_results['optimal_lr']:.2e}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Exponential LR Range Test  \n",
    "print(\"üöÄ Method 2: Exponential LR Range Test\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh model for this method\n",
    "exp_model = SampleCNN(num_classes=10, base_channels=32).to(device)\n",
    "exp_finder = ExponentialLRFinder(exp_model, criterion, optim.Adam, device)\n",
    "\n",
    "# Run exponential test\n",
    "exp_results = exp_finder.find_lr(\n",
    "    train_loader,\n",
    "    min_lr=common_params['min_lr'],\n",
    "    max_lr=common_params['max_lr'],\n",
    "    num_batches=common_params['num_batches'],\n",
    "    stop_div_factor=common_params['stop_div_factor'],\n",
    "    smooth_factor=common_params['smooth_factor']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "exp_finder.save_results(\"method_comparison_exponential\")\n",
    "all_results['Exponential'] = exp_results\n",
    "all_optimal_lrs['Exponential'] = exp_results['optimal_lr']\n",
    "\n",
    "print(f\"‚úÖ Exponential method completed. Optimal LR: {exp_results['optimal_lr']:.2e}\")\n",
    "print()\n",
    "\n",
    "# Method 3: Cyclical LR Range Test\n",
    "print(\"üîÑ Method 3: Cyclical LR Range Test\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create fresh model for this method\n",
    "cyclical_model = SampleCNN(num_classes=10, base_channels=32).to(device)\n",
    "cyclical_finder = CyclicalLRFinder(cyclical_model, criterion, optim.Adam, device)\n",
    "\n",
    "# Run cyclical test with triangular pattern\n",
    "cyclical_results = cyclical_finder.find_lr(\n",
    "    train_loader,\n",
    "    min_lr=common_params['min_lr'],\n",
    "    max_lr=common_params['max_lr'], \n",
    "    num_cycles=2,\n",
    "    cycle_pattern='triangular',\n",
    "    num_batches=common_params['num_batches'],\n",
    "    stop_div_factor=common_params['stop_div_factor'],\n",
    "    smooth_factor=common_params['smooth_factor']\n",
    ")\n",
    "\n",
    "# Save results\n",
    "cyclical_finder.save_results(\"method_comparison_cyclical\")\n",
    "all_results['Cyclical'] = cyclical_results\n",
    "all_optimal_lrs['Cyclical'] = cyclical_results['optimal_lr']\n",
    "\n",
    "print(f\"‚úÖ Cyclical method completed. Optimal LR: {cyclical_results['optimal_lr']:.2e}\")\n",
    "print()\n",
    "\n",
    "# Summary of all methods\n",
    "print(\"üìã EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<15} {'Optimal LR':<12} {'Reason'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for method_name, result in all_results.items():\n",
    "    optimal_lr = result['optimal_lr']\n",
    "    reason = result['optimal_lr_reason'][:40] + \"...\" if len(result['optimal_lr_reason']) > 40 else result['optimal_lr_reason']\n",
    "    print(f\"{method_name:<15} {optimal_lr:<12.2e} {reason}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ All LR finding methods completed successfully!\")\n",
    "print(\"üìä Results stored in all_results dictionary\")\n",
    "print(\"üéØ Optimal LRs stored in all_optimal_lrs dictionary\")\n",
    "print(\"üíæ Individual results saved to lr_finder_results/ folder\")\n",
    "\n",
    "# Quick analysis\n",
    "optimal_lrs_values = [lr for lr in all_optimal_lrs.values() if lr is not None]\n",
    "if optimal_lrs_values:\n",
    "    mean_lr = np.exp(np.mean(np.log(optimal_lrs_values)))  # Geometric mean\n",
    "    std_lr = np.std(np.log(optimal_lrs_values))  # Standard deviation in log space\n",
    "    \n",
    "    print(f\"\\\\nüìä Quick Analysis:\")\n",
    "    print(f\"   üéØ Geometric mean of optimal LRs: {mean_lr:.2e}\")\n",
    "    print(f\"   üìè Log-space std deviation: {std_lr:.3f}\")\n",
    "    print(f\"   üìà Range: {min(optimal_lrs_values):.2e} to {max(optimal_lrs_values):.2e}\")\n",
    "    \n",
    "    # Consensus recommendation\n",
    "    print(f\"\\\\nüí° Consensus Recommendation:\")\n",
    "    print(f\"   üéØ Use LR around: {mean_lr:.2e}\")\n",
    "    print(f\"   üìã Consider range: {mean_lr/3:.2e} to {mean_lr*3:.2e}\")\n",
    "\n",
    "print(\"\\\\nüéâ Ready for detailed comparison and visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc4f59",
   "metadata": {},
   "source": [
    "## 9. Compare and Visualize Results\n",
    "\n",
    "Now comes the exciting part - comparing all our LR finding methods! We'll create comprehensive visualizations that show:\n",
    "\n",
    "1. **Side-by-side comparison** of all loss curves\n",
    "2. **Optimal LR analysis** with explanations  \n",
    "3. **Method characteristics** comparison\n",
    "4. **Practical recommendations** for different scenarios\n",
    "\n",
    "### Visualization Features:\n",
    "- üìä **Unified plots**: All methods on same axes for direct comparison\n",
    "- üéØ **Optimal LR markers**: Clear indication of recommended values\n",
    "- üìà **Multiple perspectives**: Different plot types reveal different insights\n",
    "- üí° **Actionable insights**: Which method to use when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_comparison_plot(all_results, save_plot=True, show_plot=True):\n",
    "    \"\"\"\n",
    "    Create comprehensive comparison visualization of all LR finding methods\n",
    "    \"\"\"\n",
    "    print(\"üé® Creating comprehensive comparison visualization...\")\n",
    "    \n",
    "    # Set up the plot layout\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = GridSpec(4, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Color scheme for methods\n",
    "    colors = {\n",
    "        'Linear': '#2E86AB',       # Blue\n",
    "        'Exponential': '#A23B72',  # Purple \n",
    "        'Cyclical': '#F18F01'      # Orange\n",
    "    }\n",
    "    \n",
    "    # Plot 1: Main comparison - Loss vs Learning Rate\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    for method_name, results in all_results.items():\n",
    "        if results['learning_rates'] and results['smooth_losses']:\n",
    "            ax1.semilogx(results['learning_rates'], results['smooth_losses'], \n",
    "                        color=colors[method_name], linewidth=3, \n",
    "                        label=f'{method_name} (Optimal: {results[\"optimal_lr\"]:.2e})',\n",
    "                        alpha=0.8)\n",
    "            \n",
    "            # Mark optimal LR\n",
    "            if results['optimal_lr']:\n",
    "                ax1.axvline(x=results['optimal_lr'], color=colors[method_name], \n",
    "                           linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Learning Rate', fontsize=12)\n",
    "    ax1.set_ylabel('Smoothed Loss', fontsize=12)\n",
    "    ax1.set_title('LR Finder Methods Comparison: Loss vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss vs Steps for each method\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    for method_name, results in all_results.items():\n",
    "        if results['losses']:\n",
    "            steps = range(len(results['losses']))\n",
    "            ax2.plot(steps, results['smooth_losses'], color=colors[method_name], \n",
    "                    linewidth=2, label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Training Steps')\n",
    "    ax2.set_ylabel('Smoothed Loss')\n",
    "    ax2.set_title('Loss Evolution During LR Search')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning Rate Schedules\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    for method_name, results in all_results.items():\n",
    "        if results['learning_rates']:\n",
    "            steps = range(len(results['learning_rates']))\n",
    "            ax3.semilogy(steps, results['learning_rates'], color=colors[method_name], \n",
    "                        linewidth=2, label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Training Steps')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('LR Schedules Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Gradient Norms\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    for method_name, results in all_results.items():\n",
    "        if results['gradients'] and results['learning_rates']:\n",
    "            ax4.semilogx(results['learning_rates'], results['gradients'], \n",
    "                        color=colors[method_name], linewidth=2, label=method_name, alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('Learning Rate')\n",
    "    ax4.set_ylabel('Gradient Norm')\n",
    "    ax4.set_title('Gradient Norms vs LR')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Optimal LR Comparison\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    method_names = list(all_optimal_lrs.keys())\n",
    "    optimal_values = [all_optimal_lrs[name] for name in method_names if all_optimal_lrs[name] is not None]\n",
    "    method_names = [name for name in method_names if all_optimal_lrs[name] is not None]\n",
    "    \n",
    "    if optimal_values:\n",
    "        bars = ax5.bar(method_names, optimal_values, \n",
    "                      color=[colors[name] for name in method_names], alpha=0.7)\n",
    "        ax5.set_yscale('log')\n",
    "        ax5.set_ylabel('Optimal Learning Rate')\n",
    "        ax5.set_title('Optimal LR by Method')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, optimal_values):\n",
    "            height = bar.get_height()\n",
    "            ax5.text(bar.get_x() + bar.get_width()/2., height*1.1,\n",
    "                    f'{value:.1e}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 6: Method Statistics\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Calculate statistics for each method\n",
    "    stats_data = []\n",
    "    for method_name, results in all_results.items():\n",
    "        if results['losses']:\n",
    "            stats = {\n",
    "                'Method': method_name,\n",
    "                'Steps': len(results['losses']),\n",
    "                'Min Loss': min(results['smooth_losses']),\n",
    "                'Final Loss': results['smooth_losses'][-1],\n",
    "                'Loss Range': max(results['smooth_losses']) - min(results['smooth_losses'])\n",
    "            }\n",
    "            stats_data.append(stats)\n",
    "    \n",
    "    if stats_data:\n",
    "        stats_df = pd.DataFrame(stats_data)\n",
    "        \n",
    "        # Create a text summary\n",
    "        ax6.axis('off')\n",
    "        summary_text = \"üìä Method Statistics\\\\n\\\\n\"\n",
    "        \n",
    "        for _, row in stats_df.iterrows():\n",
    "            summary_text += f\"{row['Method']}:\\\\n\"\n",
    "            summary_text += f\"  Steps: {row['Steps']}\\\\n\"\n",
    "            summary_text += f\"  Min Loss: {row['Min Loss']:.4f}\\\\n\"\n",
    "            summary_text += f\"  Final Loss: {row['Final Loss']:.4f}\\\\n\"\n",
    "            summary_text += f\"  Loss Range: {row['Loss Range']:.4f}\\\\n\\\\n\"\n",
    "        \n",
    "        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, fontsize=10,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    # Plot 7: Recommendations\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    recommendations = \"\"\"\n",
    "üéØ Method Selection Guide\n",
    "\n",
    "üìà LINEAR LR RANGE TEST:\n",
    "‚úÖ First-time model training\n",
    "‚úÖ Conservative estimates\n",
    "‚úÖ Detailed exploration\n",
    "‚ùå Slow for large ranges\n",
    "\n",
    "üöÄ EXPONENTIAL LR RANGE TEST:  \n",
    "‚úÖ Quick exploration\n",
    "‚úÖ Unknown LR ranges\n",
    "‚úÖ Large models\n",
    "‚ùå May miss nuances\n",
    "\n",
    "üîÑ CYCLICAL LR RANGE TEST:\n",
    "‚úÖ Noisy datasets\n",
    "‚úÖ Robust estimates\n",
    "‚úÖ Cyclical LR planning\n",
    "‚ùå More complex analysis\n",
    "\n",
    "üí° GENERAL RECOMMENDATIONS:\n",
    "‚Ä¢ Use Linear for new models\n",
    "‚Ä¢ Use Exponential for quick tests\n",
    "‚Ä¢ Use Cyclical for confirmation\n",
    "‚Ä¢ Compare multiple methods\n",
    "\"\"\"\n",
    "    \n",
    "    ax7.text(0.05, 0.95, recommendations, transform=ax7.transAxes, fontsize=9,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    # Plot 8-9: Detailed method analysis\n",
    "    ax8 = fig.add_subplot(gs[3, :2])\n",
    "    \n",
    "    # Create loss derivative analysis\n",
    "    for method_name, results in all_results.items():\n",
    "        if len(results['smooth_losses']) > 10:\n",
    "            # Calculate loss derivatives\n",
    "            losses = np.array(results['smooth_losses'])\n",
    "            lrs = np.array(results['learning_rates'])\n",
    "            \n",
    "            # Use log scale for LR\n",
    "            log_lrs = np.log10(lrs)\n",
    "            derivatives = np.gradient(losses, log_lrs)\n",
    "            \n",
    "            ax8.semilogx(lrs[1:-1], derivatives[1:-1], color=colors[method_name], \n",
    "                        linewidth=2, label=f'{method_name} Loss Derivative', alpha=0.8)\n",
    "    \n",
    "    ax8.set_xlabel('Learning Rate')\n",
    "    ax8.set_ylabel('Loss Derivative (d_loss/d_log_lr)')\n",
    "    ax8.set_title('Loss Derivatives: Finding Steepest Descent')\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    ax8.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Plot 9: Summary insights\n",
    "    ax9 = fig.add_subplot(gs[3, 2])\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    # Calculate consensus\n",
    "    if optimal_values:\n",
    "        mean_lr = np.exp(np.mean(np.log(optimal_values)))\n",
    "        \n",
    "        consensus_text = f\"\"\"\n",
    "üìã EXPERIMENT SUMMARY\n",
    "\n",
    "üéØ Optimal LRs Found:\n",
    "\"\"\"\n",
    "        for method, lr in all_optimal_lrs.items():\n",
    "            if lr:\n",
    "                consensus_text += f\"  {method}: {lr:.2e}\\\\n\"\n",
    "        \n",
    "        consensus_text += f\"\"\"\n",
    "üìä Consensus Analysis:\n",
    "  Geometric Mean: {mean_lr:.2e}\n",
    "  Recommended Range: \n",
    "    {mean_lr/3:.2e} to {mean_lr*3:.2e}\n",
    "\n",
    "üí° Final Recommendation:\n",
    "  Start with: {mean_lr:.2e}\n",
    "  Monitor and adjust based on:\n",
    "  ‚Ä¢ Training stability\n",
    "  ‚Ä¢ Convergence speed  \n",
    "  ‚Ä¢ Validation performance\n",
    "\"\"\"\n",
    "        \n",
    "        ax9.text(0.05, 0.95, consensus_text, transform=ax9.transAxes, fontsize=9,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Universal Learning Rate Finder: Comprehensive Method Comparison', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    if save_plot:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plot_path = BASE_DIR / 'plots' / f'comprehensive_lr_comparison_{timestamp}.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üíæ Comprehensive comparison plot saved to: {plot_path}\")\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the comprehensive comparison\n",
    "comparison_fig = create_comprehensive_comparison_plot(all_results, save_plot=True, show_plot=True)\n",
    "\n",
    "print(\"\\\\n‚úÖ Comprehensive comparison visualization created!\")\n",
    "print(\"üìä All methods compared side-by-side with detailed analysis\")\n",
    "print(\"üéØ Optimal LR recommendations provided\")\n",
    "print(\"üí° Method selection guide included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e0f99",
   "metadata": {},
   "source": [
    "## 10. Save Results and Models\n",
    "\n",
    "Finally, let's organize and save all our experimental results, trained models, and analysis for future reference and reproducibility.\n",
    "\n",
    "### What We'll Save:\n",
    "1. **Experimental Results**: All LR finder data and metadata\n",
    "2. **Comparison Analysis**: Summary statistics and recommendations  \n",
    "3. **Model Checkpoints**: Trained model states for reproduction\n",
    "4. **Configuration Files**: Complete experimental setup\n",
    "5. **Documentation**: Usage guide and findings summary\n",
    "\n",
    "### Organization:\n",
    "- üìÅ **Structured storage** in organized subfolders\n",
    "- üè∑Ô∏è **Clear naming** with timestamps and method identifiers\n",
    "- üìã **Metadata preservation** for complete reproducibility\n",
    "- üìä **Summary reports** for easy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da023f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_comprehensive_results(all_results, all_optimal_lrs, sample_models):\n",
    "    \"\"\"\n",
    "    Save all experimental results, models, and analysis\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"üíæ Saving comprehensive experiment results...\")\n",
    "    print(f\"üïí Timestamp: {timestamp}\")\n",
    "    \n",
    "    # 1. Save comprehensive experiment summary\n",
    "    experiment_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'experiment_info': {\n",
    "            'purpose': 'Universal Learning Rate Finder Comparison',\n",
    "            'methods_tested': list(all_results.keys()),\n",
    "            'total_methods': len(all_results),\n",
    "            'dataset_info': {\n",
    "                'type': 'CIFAR-10 or Synthetic',\n",
    "                'samples': len(train_dataset),\n",
    "                'batches': len(train_loader),\n",
    "                'batch_size': train_loader.batch_size\n",
    "            },\n",
    "            'model_info': {\n",
    "                'architecture': 'SampleCNN',\n",
    "                'parameters': sum(p.numel() for p in demo_model.parameters()),\n",
    "                'device': str(device)\n",
    "            }\n",
    "        },\n",
    "        'optimal_lrs': all_optimal_lrs,\n",
    "        'method_comparison': {},\n",
    "        'recommendations': {}\n",
    "    }\n",
    "    \n",
    "    # 2. Add detailed method comparison\n",
    "    for method_name, results in all_results.items():\n",
    "        method_stats = {\n",
    "            'optimal_lr': results['optimal_lr'],\n",
    "            'optimal_lr_reason': results['optimal_lr_reason'],\n",
    "            'total_steps': len(results['learning_rates']),\n",
    "            'lr_range': {\n",
    "                'min': min(results['learning_rates']) if results['learning_rates'] else None,\n",
    "                'max': max(results['learning_rates']) if results['learning_rates'] else None\n",
    "            },\n",
    "            'loss_stats': {\n",
    "                'min': min(results['smooth_losses']) if results['smooth_losses'] else None,\n",
    "                'max': max(results['smooth_losses']) if results['smooth_losses'] else None,\n",
    "                'final': results['smooth_losses'][-1] if results['smooth_losses'] else None\n",
    "            },\n",
    "            'experiment_settings': results.get('experiment_info', {})\n",
    "        }\n",
    "        experiment_summary['method_comparison'][method_name] = method_stats\n",
    "    \n",
    "    # 3. Generate consensus recommendations\n",
    "    optimal_values = [lr for lr in all_optimal_lrs.values() if lr is not None]\n",
    "    if optimal_values:\n",
    "        mean_lr = np.exp(np.mean(np.log(optimal_values)))\n",
    "        std_lr = np.std(np.log(optimal_values))\n",
    "        \n",
    "        experiment_summary['recommendations'] = {\n",
    "            'consensus_lr': mean_lr,\n",
    "            'recommended_range': {\n",
    "                'conservative': mean_lr / 3,\n",
    "                'aggressive': mean_lr * 3\n",
    "            },\n",
    "            'log_std': std_lr,\n",
    "            'agreement_level': 'High' if std_lr < 0.5 else 'Medium' if std_lr < 1.0 else 'Low',\n",
    "            'suggestions': {\n",
    "                'start_with': mean_lr,\n",
    "                'monitor_for': ['training_stability', 'convergence_speed', 'validation_performance'],\n",
    "                'adjust_based_on': 'loss_behavior_and_gradient_norms'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Save experiment summary\n",
    "    summary_path = BASE_DIR / 'experiments' / f'lr_finder_experiment_summary_{timestamp}.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(experiment_summary, f, indent=2)\n",
    "    print(f\"üìã Experiment summary saved: {summary_path}\")\n",
    "    \n",
    "    # 4. Save individual method results (detailed)\n",
    "    results_dir = BASE_DIR / 'experiments' / f'detailed_results_{timestamp}'\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for method_name, results in all_results.items():\n",
    "        # Save as both JSON and CSV\n",
    "        method_data = {\n",
    "            'learning_rates': results['learning_rates'],\n",
    "            'losses': results['losses'],\n",
    "            'smooth_losses': results['smooth_losses'],\n",
    "            'gradients': results['gradients']\n",
    "        }\n",
    "        \n",
    "        # JSON format (complete data)\n",
    "        json_path = results_dir / f'{method_name.lower()}_complete_data.json'\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(method_data, f, indent=2)\n",
    "        \n",
    "        # CSV format (for analysis)\n",
    "        if method_data['learning_rates']:\n",
    "            df = pd.DataFrame(method_data)\n",
    "            csv_path = results_dir / f'{method_name.lower()}_data.csv'\n",
    "            df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"üìä Detailed results saved: {results_dir}\")\n",
    "    \n",
    "    # 5. Save model checkpoints\n",
    "    models_dir = BASE_DIR / 'models' / f'lr_finder_models_{timestamp}'\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for model_name, model in sample_models.items():\n",
    "        model_path = models_dir / f'{model_name}_{timestamp}.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'model_params': {\n",
    "                'num_classes': 10,\n",
    "                'base_channels': getattr(model, 'conv1', {}).get('out_channels', 32) if hasattr(model, 'conv1') else 32\n",
    "            },\n",
    "            'experiment_timestamp': timestamp\n",
    "        }, model_path)\n",
    "    \n",
    "    print(f\"üèóÔ∏è Model checkpoints saved: {models_dir}\")\n",
    "    \n",
    "    # 6. Create usage documentation\n",
    "    usage_doc = f\\\"\\\"\\\"# Learning Rate Finder Experiment Results\n",
    "    \n",
    "## Experiment Information\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Methods Tested**: {', '.join(all_results.keys())}\n",
    "- **Dataset**: CIFAR-10 or Synthetic ({len(train_dataset)} samples)\n",
    "- **Model**: SampleCNN ({sum(p.numel() for p in demo_model.parameters()):,} parameters)\n",
    "- **Device**: {device}\n",
    "\n",
    "## Optimal Learning Rates Found\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "    \n",
    "    for method, lr in all_optimal_lrs.items():\n",
    "        if lr:\n",
    "            usage_doc += f\\\"- **{method}**: {lr:.2e}\\\\n\\\"\n",
    "    \n",
    "    if optimal_values:\n",
    "        usage_doc += f\\\"\\\"\\\"\n",
    "## Consensus Recommendation\n",
    "\n",
    "- **Recommended LR**: {mean_lr:.2e}\n",
    "- **Conservative Range**: {mean_lr/3:.2e} to {mean_lr*3:.2e}\n",
    "- **Agreement Level**: {experiment_summary['recommendations']['agreement_level']}\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Quick Start\n",
    "```python\n",
    "# Use the consensus LR for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr={mean_lr:.2e})\n",
    "```\n",
    "\n",
    "### Method-Specific Usage\n",
    "```python\n",
    "# Linear method result\n",
    "linear_lr = {all_optimal_lrs.get('Linear', 'N/A')}\n",
    "\n",
    "# Exponential method result  \n",
    "exp_lr = {all_optimal_lrs.get('Exponential', 'N/A')}\n",
    "\n",
    "# Cyclical method result\n",
    "cyclical_lr = {all_optimal_lrs.get('Cyclical', 'N/A')}\n",
    "```\n",
    "\n",
    "### Advanced Usage\n",
    "```python\n",
    "# Load and use the LR finder results\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load experiment summary\n",
    "with open('experiments/lr_finder_experiment_summary_{timestamp}.json') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "# Load detailed method data\n",
    "linear_data = pd.read_csv('experiments/detailed_results_{timestamp}/linear_data.csv')\n",
    "exp_data = pd.read_csv('experiments/detailed_results_{timestamp}/exponential_data.csv')\n",
    "cyclical_data = pd.read_csv('experiments/detailed_results_{timestamp}/cyclical_data.csv')\n",
    "\n",
    "# Use in your training loop\n",
    "recommended_lr = summary['recommendations']['consensus_lr']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=recommended_lr)\n",
    "```\n",
    "\n",
    "## Files Created\n",
    "\n",
    "### Results\n",
    "- `lr_finder_experiment_summary_{timestamp}.json` - Complete experiment summary\n",
    "- `detailed_results_{timestamp}/` - Individual method data (JSON + CSV)\n",
    "\n",
    "### Models\n",
    "- `lr_finder_models_{timestamp}/` - Model checkpoints for reproduction\n",
    "\n",
    "### Visualizations  \n",
    "- `plots/comprehensive_lr_comparison_{timestamp}.png` - Main comparison plot\n",
    "- `plots/[method]_lr_test_{timestamp}.png` - Individual method plots\n",
    "\n",
    "## Recommendations for Future Use\n",
    "\n",
    "1. **For New Models**: Start with Linear LR Range Test\n",
    "2. **For Quick Tests**: Use Exponential LR Range Test  \n",
    "3. **For Robust Estimates**: Use Cyclical LR Range Test\n",
    "4. **For Best Results**: Compare multiple methods like this experiment\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All results are reproducible using the saved model checkpoints\n",
    "- Experiment used consistent parameters across all methods for fair comparison\n",
    "- Results may vary with different datasets, models, or hyperparameters\n",
    "- Consider dataset-specific fine-tuning of LR finder parameters\n",
    "\n",
    "Generated by Universal Learning Rate Finder Toolkit\n",
    "\\\"\\\"\\\"\n",
    "    \n",
    "    doc_path = BASE_DIR / 'experiments' / f'lr_finder_usage_guide_{timestamp}.md'\n",
    "    with open(doc_path, 'w') as f:\n",
    "        f.write(usage_doc)\n",
    "    print(f\"üìö Usage documentation saved: {doc_path}\")\n",
    "    \n",
    "    # 7. Create a simple results CSV for quick reference\n",
    "    quick_ref_data = {\n",
    "        'Method': [],\n",
    "        'Optimal_LR': [],\n",
    "        'Min_Loss': [],\n",
    "        'Total_Steps': [],\n",
    "        'Status': []\n",
    "    }\n",
    "    \n",
    "    for method_name, results in all_results.items():\n",
    "        quick_ref_data['Method'].append(method_name)\n",
    "        quick_ref_data['Optimal_LR'].append(results['optimal_lr'])\n",
    "        quick_ref_data['Min_Loss'].append(min(results['smooth_losses']) if results['smooth_losses'] else None)\n",
    "        quick_ref_data['Total_Steps'].append(len(results['learning_rates']))\n",
    "        quick_ref_data['Status'].append('Success' if results['optimal_lr'] else 'Failed')\n",
    "    \n",
    "    quick_ref_df = pd.DataFrame(quick_ref_data)\n",
    "    quick_ref_path = BASE_DIR / 'experiments' / f'lr_finder_quick_reference_{timestamp}.csv'\n",
    "    quick_ref_df.to_csv(quick_ref_path, index=False)\n",
    "    print(f\"üìä Quick reference saved: {quick_ref_path}\")\n",
    "    \n",
    "    print(f\\\"\\\\n‚úÖ All results saved successfully!\\\")\n",
    "    print(f\\\"üìÅ Main directory: {BASE_DIR}\\\")\n",
    "    print(f\\\"üéâ Experiment {timestamp} complete and documented!\\\")\n",
    "    \n",
    "    return {\n",
    "        'summary_path': summary_path,\n",
    "        'results_dir': results_dir,\n",
    "        'models_dir': models_dir,\n",
    "        'doc_path': doc_path,\n",
    "        'quick_ref_path': quick_ref_path,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "\n",
    "# Save all results\n",
    "saved_paths = save_comprehensive_results(all_results, all_optimal_lrs, sample_models)\n",
    "\n",
    "# Final summary\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\n",
    "print(\\\"üéâ UNIVERSAL LEARNING RATE FINDER TOOLKIT - EXPERIMENT COMPLETE! üéâ\\\")\n",
    "print(\\\"=\\\" * 80)\n",
    "\n",
    "print(f\\\"\\\\nüìä EXPERIMENT SUMMARY:\\\")\n",
    "print(f\\\"   üîç Methods tested: {len(all_results)}\\\")\n",
    "print(f\\\"   üéØ Optimal LRs found: {len([lr for lr in all_optimal_lrs.values() if lr])}\\\")\n",
    "print(f\\\"   üìÅ Files created: {len(saved_paths) - 1}\\\")  # -1 for timestamp\n",
    "print(f\\\"   üïí Experiment ID: {saved_paths['timestamp']}\\\")\n",
    "\n",
    "print(f\\\"\\\\nüéØ OPTIMAL LEARNING RATES:\\\")\n",
    "for method, lr in all_optimal_lrs.items():\n",
    "    if lr:\n",
    "        print(f\\\"   {method}: {lr:.2e}\\\")\n",
    "\n",
    "if len([lr for lr in all_optimal_lrs.values() if lr]) > 1:\n",
    "    optimal_values = [lr for lr in all_optimal_lrs.values() if lr is not None]\n",
    "    mean_lr = np.exp(np.mean(np.log(optimal_values)))\n",
    "    print(f\\\"   üìà Consensus: {mean_lr:.2e}\\\")\n",
    "\n",
    "print(f\\\"\\\\nüí° NEXT STEPS:\\\")\n",
    "print(f\\\"   1. Review the comprehensive comparison plot\\\")\n",
    "print(f\\\"   2. Check the usage guide: {saved_paths['doc_path'].name}\\\")\n",
    "print(f\\\"   3. Use optimal LRs in your training\\\")\n",
    "print(f\\\"   4. Experiment with different models/datasets\\\")\n",
    "\n",
    "print(f\\\"\\\\nüöÄ TOOLKIT FEATURES DEMONSTRATED:\\\")\n",
    "print(f\\\"   ‚úÖ Universal model compatibility\\\")\n",
    "print(f\\\"   ‚úÖ Multiple LR finding methods\\\") \n",
    "print(f\\\"   ‚úÖ Comprehensive comparison analysis\\\")\n",
    "print(f\\\"   ‚úÖ Automated optimal LR detection\\\")\n",
    "print(f\\\"   ‚úÖ Professional result organization\\\")\n",
    "print(f\\\"   ‚úÖ Complete reproducibility\\\")\n",
    "\n",
    "print(f\\\"\\\\nüìö Happy Learning Rate Optimization! üîçüìà\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d6235",
   "metadata": {},
   "source": [
    "## üéØ Conclusion & Next Steps\n",
    "\n",
    "**Congratulations! You've built a comprehensive Universal Learning Rate Finder Toolkit!**\n",
    "\n",
    "### üåü What You've Accomplished:\n",
    "\n",
    "1. **üìö Complete LR Finding Arsenal**:\n",
    "   - Linear LR Range Test (conservative, thorough)\n",
    "   - Exponential LR Range Test (fast, aggressive)  \n",
    "   - Cyclical LR Range Test (robust, pattern-based)\n",
    "\n",
    "2. **üîß Universal Compatibility**:\n",
    "   - Works with ANY PyTorch model\n",
    "   - Supports any dataset format\n",
    "   - Flexible architecture handling\n",
    "\n",
    "3. **üìä Professional Analysis**:\n",
    "   - Comprehensive comparison visualizations\n",
    "   - Automated optimal LR detection\n",
    "   - Statistical consensus recommendations\n",
    "\n",
    "4. **üíæ Production Ready**:\n",
    "   - Organized result storage\n",
    "   - Complete reproducibility\n",
    "   - Professional documentation\n",
    "\n",
    "### üéØ Key Features Delivered:\n",
    "\n",
    "‚úÖ **Model-Agnostic**: Works with CNNs, RNNs, Transformers, custom architectures\n",
    "‚úÖ **Multiple Methods**: 3 different LR finding approaches for robust analysis  \n",
    "‚úÖ **Smart Recommendations**: Automated optimal LR detection with explanations\n",
    "‚úÖ **Beautiful Visualizations**: Comprehensive plots for easy interpretation\n",
    "‚úÖ **Organized Structure**: Professional experiment tracking and storage\n",
    "‚úÖ **Complete Documentation**: Usage guides and reproducible workflows\n",
    "\n",
    "### üöÄ How to Use This Toolkit:\n",
    "\n",
    "#### **Quick Start** (any model):\n",
    "```python\n",
    "# 1. Choose your model and data\n",
    "model = YourModel()\n",
    "train_loader = YourDataLoader()\n",
    "\n",
    "# 2. Pick a method and find optimal LR\n",
    "finder = LinearLRFinder(model, nn.CrossEntropyLoss())\n",
    "results = finder.find_lr(train_loader)\n",
    "\n",
    "# 3. Use the optimal LR\n",
    "optimal_lr = results['optimal_lr']\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=optimal_lr)\n",
    "```\n",
    "\n",
    "#### **Comprehensive Analysis**:\n",
    "```python\n",
    "# Compare all methods for best results\n",
    "linear_finder = LinearLRFinder(model, criterion)\n",
    "exp_finder = ExponentialLRFinder(model, criterion)  \n",
    "cyclical_finder = CyclicalLRFinder(model, criterion)\n",
    "\n",
    "# Get consensus recommendation\n",
    "all_results = {}\n",
    "all_results['Linear'] = linear_finder.find_lr(train_loader)\n",
    "all_results['Exponential'] = exp_finder.find_lr(train_loader)\n",
    "all_results['Cyclical'] = cyclical_finder.find_lr(train_loader)\n",
    "```\n",
    "\n",
    "### üí° When to Use Each Method:\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Linear** | New models, detailed analysis | Conservative, thorough | Slower for large ranges |\n",
    "| **Exponential** | Quick tests, unknown ranges | Fast, covers wide ranges | May miss fine details |\n",
    "| **Cyclical** | Noisy data, robust estimates | Multiple confirmations | More complex analysis |\n",
    "\n",
    "### üî¨ Advanced Applications:\n",
    "\n",
    "1. **Research Projects**: Compare LR sensitivity across architectures\n",
    "2. **Production Models**: Establish optimal LR baselines  \n",
    "3. **Hyperparameter Tuning**: Integrate with automated search\n",
    "4. **Educational Use**: Teach LR optimization concepts\n",
    "5. **Debugging**: Diagnose training instability issues\n",
    "\n",
    "### üìà Future Enhancements:\n",
    "\n",
    "Consider extending this toolkit with:\n",
    "- **Warmup LR schedules** integration\n",
    "- **Multi-GPU** support for large models\n",
    "- **Learning rate scheduling** recommendations\n",
    "- **Automatic hyperparameter** optimization\n",
    "- **Integration with popular frameworks** (Lightning, Transformers)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You now have a professional-grade learning rate optimization toolkit that rivals commercial solutions!**\n",
    "\n",
    "**Happy training! üöÄüìä**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
