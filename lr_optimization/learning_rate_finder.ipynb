{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f4c055",
   "metadata": {},
   "source": [
    "# Learning Rate Finder üîç\n",
    "\n",
    "This notebook implements the Learning Rate Range Test (LR Finder) to help you find the optimal learning rate for training your ResNet50 model on ImageNet datasets.\n",
    "\n",
    "## What is Learning Rate Finder?\n",
    "\n",
    "The Learning Rate Finder is a technique that:\n",
    "- **Systematically tests** different learning rates during training\n",
    "- **Plots loss vs learning rate** to visualize the relationship\n",
    "- **Identifies the optimal range** where loss decreases most rapidly\n",
    "- **Prevents poor convergence** due to suboptimal learning rates\n",
    "\n",
    "## Key Benefits:\n",
    "- üéØ **Find optimal LR** - Discover the best learning rate for your specific setup\n",
    "- ‚ö° **Faster convergence** - Start training with an ideal learning rate\n",
    "- üìä **Visual guidance** - Clear plots show the optimal range\n",
    "- üõ°Ô∏è **Avoid pitfalls** - Prevent too high/low learning rates\n",
    "\n",
    "## How to Use:\n",
    "1. Update dataset path and model configuration\n",
    "2. Run the learning rate finder\n",
    "3. Analyze the loss vs LR plot\n",
    "4. Choose the learning rate where loss decreases most steeply\n",
    "5. Use this LR for your actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "parent_dir = Path('../').resolve()\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "# Import our custom modules\n",
    "from imagenet_models import resnet50_imagenet\n",
    "from imagenet_dataset import get_imagenet_transforms\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Setup\n",
    "# ‚ö†Ô∏è UPDATE THESE PATHS TO YOUR DATASET LOCATION\n",
    "IMAGENET_ROOT = \"../datasets/tiny-imagenet-200\"  # Default to Tiny ImageNet\n",
    "TRAIN_DIR = str(Path(IMAGENET_ROOT) / \"train\")\n",
    "VAL_DIR = str(Path(IMAGENET_ROOT) / \"val\")\n",
    "\n",
    "# Check if dataset exists, if not suggest alternatives\n",
    "if not Path(IMAGENET_ROOT).exists():\n",
    "    print(\"‚ö†Ô∏è Dataset not found at default location!\")\n",
    "    print(\"\\nüìÅ Available dataset options:\")\n",
    "    datasets_dir = Path(\"../datasets\")\n",
    "    if datasets_dir.exists():\n",
    "        for dataset_path in datasets_dir.iterdir():\n",
    "            if dataset_path.is_dir():\n",
    "                print(f\"   ‚Ä¢ {dataset_path.name}\")\n",
    "    else:\n",
    "        print(\"   üì• No datasets found. Use dataset_tools/imagenet_subset_downloader.ipynb to download one!\")\n",
    "    \n",
    "    print(\"\\nüîß To use a different dataset, update IMAGENET_ROOT above\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset found: {IMAGENET_ROOT}\")\n",
    "\n",
    "# Learning Rate Finder Configuration\n",
    "LR_FINDER_CONFIG = {\n",
    "    'min_lr': 1e-7,           # Minimum learning rate to test\n",
    "    'max_lr': 10.0,           # Maximum learning rate to test\n",
    "    'num_iterations': 100,    # Number of iterations to run\n",
    "    'beta': 0.98,             # Smoothing factor for loss\n",
    "    'stop_div_factor': 5,     # Stop if loss increases by this factor\n",
    "    'batch_size': 128,        # Batch size for LR finder\n",
    "    'num_workers': 4,         # Number of data loading workers\n",
    "    'subset_size': 5000,      # Use subset of data for faster testing\n",
    "}\n",
    "\n",
    "# Auto-detect dataset type and adjust configuration\n",
    "if \"tiny-imagenet\" in IMAGENET_ROOT.lower():\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 200,       # Tiny ImageNet has 200 classes\n",
    "        'pretrained': False,      # No pretrained for Tiny ImageNet\n",
    "        'input_size': 64,         # 64x64 images\n",
    "    }\n",
    "    LR_FINDER_CONFIG['batch_size'] = 256  # Can use larger batch for smaller images\n",
    "elif \"imagenette\" in IMAGENET_ROOT.lower() or \"imagewoof\" in IMAGENET_ROOT.lower():\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 10,        # Imagenette/ImageWoof have 10 classes\n",
    "        'pretrained': True,       # Pretrained recommended for smaller datasets\n",
    "        'input_size': 224,        # Standard ImageNet size\n",
    "    }\n",
    "else:\n",
    "    # Full ImageNet or custom dataset\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 1000,      # Full ImageNet classes\n",
    "        'pretrained': False,      # Set to True if you want pretrained weights\n",
    "        'input_size': 224,        # Standard ImageNet size\n",
    "    }\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"‚ö†Ô∏è Warning: Using CPU. LR finder will be slow.\")\n",
    "    LR_FINDER_CONFIG['batch_size'] = 32  # Smaller batch for CPU\n",
    "    LR_FINDER_CONFIG['subset_size'] = 1000  # Smaller subset for CPU\n",
    "\n",
    "print(\"\\nüìã LR Finder Configuration:\")\n",
    "for key, value in LR_FINDER_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Model Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\"Learning Rate Range Test Implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        # Store initial model and optimizer state\n",
    "        self.model_state = copy.deepcopy(model.state_dict())\n",
    "        self.optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "        \n",
    "        # Results storage\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset model and optimizer to initial state\"\"\"\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def find_lr(self, train_loader, min_lr=1e-7, max_lr=10.0, num_iterations=100, \n",
    "                beta=0.98, stop_div_factor=5):\n",
    "        \"\"\"Perform learning rate range test\"\"\"\n",
    "        \n",
    "        print(f\"üîç Starting Learning Rate Finder...\")\n",
    "        print(f\"   üìä LR range: {min_lr:.2e} to {max_lr:.2e}\")\n",
    "        print(f\"   üîÑ Iterations: {num_iterations}\")\n",
    "        \n",
    "        # Reset to initial state\n",
    "        self.reset()\n",
    "        \n",
    "        # Calculate learning rate schedule\n",
    "        lr_schedule = np.logspace(np.log10(min_lr), np.log10(max_lr), num_iterations)\n",
    "        \n",
    "        # Training setup\n",
    "        self.model.train()\n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        smoothed_loss = 0\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(enumerate(lr_schedule), total=num_iterations, desc=\"LR Finder\")\n",
    "        \n",
    "        for i, lr in pbar:\n",
    "            # Update learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # Get next batch (cycle through dataset if needed)\n",
    "            try:\n",
    "                inputs, targets = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                inputs, targets = next(data_iter)\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "            # Smooth loss for better visualization\n",
    "            if i == 0:\n",
    "                smoothed_loss = current_loss\n",
    "            else:\n",
    "                smoothed_loss = beta * smoothed_loss + (1 - beta) * current_loss\n",
    "            \n",
    "            # Store values\n",
    "            self.learning_rates.append(lr)\n",
    "            self.losses.append(smoothed_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'LR': f'{lr:.2e}',\n",
    "                'Loss': f'{smoothed_loss:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping if loss explodes\n",
    "            if i > 0 and smoothed_loss > stop_div_factor * best_loss:\n",
    "                print(f\"\\nüõë Stopping early at iteration {i}: loss exploded!\")\n",
    "                break\n",
    "            \n",
    "            # Track best loss\n",
    "            if smoothed_loss < best_loss:\n",
    "                best_loss = smoothed_loss\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"‚úÖ Learning rate finder completed!\")\n",
    "        print(f\"   üìä Tested {len(self.learning_rates)} learning rates\")\n",
    "        print(f\"   üìâ Best loss: {best_loss:.4f}\")\n",
    "        \n",
    "        return self.learning_rates, self.losses\n",
    "    \n",
    "    def plot_results(self, skip_start=10, skip_end=5, suggest_lr=True):\n",
    "        \"\"\"Plot learning rate vs loss with suggestions\"\"\"\n",
    "        \n",
    "        if len(self.learning_rates) == 0:\n",
    "            print(\"‚ùå No results to plot. Run find_lr() first!\")\n",
    "            return\n",
    "        \n",
    "        # Skip noisy start and end portions\n",
    "        start_idx = skip_start\n",
    "        end_idx = len(self.learning_rates) - skip_end\n",
    "        \n",
    "        lr_plot = self.learning_rates[start_idx:end_idx]\n",
    "        loss_plot = self.losses[start_idx:end_idx]\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Main plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.semilogx(lr_plot, loss_plot, 'b-', linewidth=2, label='Loss vs LR')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder Results', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Suggest optimal learning rate\n",
    "        if suggest_lr and len(loss_plot) > 10:\n",
    "            # Find steepest descent (negative gradient)\n",
    "            gradients = np.gradient(loss_plot)\n",
    "            min_gradient_idx = np.argmin(gradients)\n",
    "            suggested_lr = lr_plot[min_gradient_idx]\n",
    "            \n",
    "            # Also find minimum loss point for reference\n",
    "            min_loss_idx = np.argmin(loss_plot)\n",
    "            min_loss_lr = lr_plot[min_loss_idx]\n",
    "            \n",
    "            # Plot suggestions\n",
    "            plt.axvline(suggested_lr, color='red', linestyle='--', alpha=0.8, \n",
    "                       label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "            plt.axvline(min_loss_lr, color='green', linestyle='--', alpha=0.8,\n",
    "                       label=f'Min Loss LR: {min_loss_lr:.2e}')\n",
    "            \n",
    "            # Conservative suggestion (1/10 of steepest point)\n",
    "            conservative_lr = suggested_lr / 10\n",
    "            plt.axvline(conservative_lr, color='orange', linestyle='--', alpha=0.8,\n",
    "                       label=f'Conservative LR: {conservative_lr:.2e}')\n",
    "        \n",
    "        plt.legend()\n",
    "        \n",
    "        # Gradient plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        if len(loss_plot) > 1:\n",
    "            gradients = np.gradient(loss_plot)\n",
    "            plt.semilogx(lr_plot, gradients, 'r-', linewidth=2, label='Loss Gradient')\n",
    "            plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Loss Gradient')\n",
    "            plt.title('Loss Gradient (Choose LR where gradient is most negative)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print recommendations\n",
    "        if suggest_lr and len(loss_plot) > 10:\n",
    "            print(\"\\nüéØ Learning Rate Recommendations:\")\n",
    "            print(f\"   üî¥ Steepest descent: {suggested_lr:.2e} (where loss decreases fastest)\")\n",
    "            print(f\"   üü¢ Minimum loss: {min_loss_lr:.2e} (lowest loss achieved)\")\n",
    "            print(f\"   üü† Conservative: {conservative_lr:.2e} (safer choice, 1/10 of steepest)\")\n",
    "            print(\"\\nüí° Recommended approach:\")\n",
    "            print(f\"   ‚Ä¢ Start with conservative LR: {conservative_lr:.2e}\")\n",
    "            print(f\"   ‚Ä¢ If training is stable, try steepest: {suggested_lr:.2e}\")\n",
    "            print(f\"   ‚Ä¢ Monitor loss and adjust as needed\")\n",
    "    \n",
    "    def export_results(self, filename='lr_finder_results.txt'):\n",
    "        \"\"\"Export results to file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"Learning Rate Finder Results\\n\")\n",
    "            f.write(\"===========================\\n\\n\")\n",
    "            for lr, loss in zip(self.learning_rates, self.losses):\n",
    "                f.write(f\"{lr:.6e}\\t{loss:.6f}\\n\")\n",
    "        print(f\"üìÑ Results exported to {filename}\")\n",
    "\n",
    "print(\"üèóÔ∏è LearningRateFinder class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset and DataLoader\n",
    "print(\"üìä Preparing dataset...\")\n",
    "\n",
    "# Get transforms based on input size\n",
    "if MODEL_CONFIG['input_size'] == 64:\n",
    "    # Tiny ImageNet transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "else:\n",
    "    # Standard ImageNet transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_DIR,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"   üìä Total samples: {len(train_dataset):,}\")\n",
    "    print(f\"   üè∑Ô∏è Number of classes: {len(train_dataset.classes)}\")\n",
    "    \n",
    "    # Create subset for faster LR finding\n",
    "    subset_size = min(LR_FINDER_CONFIG['subset_size'], len(train_dataset))\n",
    "    subset_indices = torch.randperm(len(train_dataset))[:subset_size]\n",
    "    train_subset = Subset(train_dataset, subset_indices)\n",
    "    \n",
    "    print(f\"   üéØ Using subset: {len(train_subset):,} samples\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=LR_FINDER_CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=LR_FINDER_CONFIG['num_workers'],\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"   üì¶ Batch size: {LR_FINDER_CONFIG['batch_size']}\")\n",
    "    print(f\"   üîÑ Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check if dataset path is correct\")\n",
    "    print(\"   2. Ensure dataset has train/ subdirectory\")\n",
    "    print(\"   3. Verify dataset structure (train/class_name/images.jpg)\")\n",
    "    print(\"   4. Download dataset using: ../dataset_tools/imagenet_subset_downloader.ipynb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model, Optimizer, and Loss Function\n",
    "print(\"üèóÔ∏è Initializing model...\")\n",
    "\n",
    "# Create model\n",
    "model = resnet50_imagenet(\n",
    "    num_classes=MODEL_CONFIG['num_classes'],\n",
    "    pretrained=MODEL_CONFIG['pretrained']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model created: {MODEL_CONFIG['model_name']}\")\n",
    "print(f\"   üéØ Classes: {MODEL_CONFIG['num_classes']}\")\n",
    "print(f\"   üîÑ Pretrained: {MODEL_CONFIG['pretrained']}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   üìä Total parameters: {total_params:,}\")\n",
    "print(f\"   üéì Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Initialize optimizer (SGD is commonly used for LR finding)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR_FINDER_CONFIG['min_lr'],  # Will be updated during LR finding\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"‚úÖ Optimizer: SGD with momentum=0.9, weight_decay=1e-4\")\n",
    "print(f\"‚úÖ Loss function: CrossEntropyLoss\")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        # Create dummy input based on input size\n",
    "        dummy_input = torch.randn(2, 3, MODEL_CONFIG['input_size'], MODEL_CONFIG['input_size']).to(device)\n",
    "        dummy_output = model(dummy_input)\n",
    "        print(f\"‚úÖ Forward pass test successful!\")\n",
    "        print(f\"   üìä Input shape: {dummy_input.shape}\")\n",
    "        print(f\"   üìä Output shape: {dummy_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Forward pass test failed: {e}\")\n",
    "        raise\n",
    "\n",
    "model.train()  # Set back to training mode\n",
    "print(\"\\nüöÄ Ready for learning rate finding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning Rate Finder\n",
    "print(\"üîç Running Learning Rate Finder...\")\n",
    "print(\"\" * 50)\n",
    "\n",
    "# Create LR Finder instance\n",
    "lr_finder = LearningRateFinder(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Run the learning rate range test\n",
    "learning_rates, losses = lr_finder.find_lr(\n",
    "    train_loader=train_loader,\n",
    "    min_lr=LR_FINDER_CONFIG['min_lr'],\n",
    "    max_lr=LR_FINDER_CONFIG['max_lr'],\n",
    "    num_iterations=LR_FINDER_CONFIG['num_iterations'],\n",
    "    beta=LR_FINDER_CONFIG['beta'],\n",
    "    stop_div_factor=LR_FINDER_CONFIG['stop_div_factor']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ Learning Rate Finder completed!\")\n",
    "print(f\"üìä Tested {len(learning_rates)} learning rates\")\n",
    "print(f\"üìâ Loss range: {min(losses):.4f} to {max(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and Analyze Results\n",
    "print(\"üìä Plotting results and analyzing optimal learning rate...\")\n",
    "\n",
    "# Plot the results with suggestions\n",
    "lr_finder.plot_results(\n",
    "    skip_start=10,    # Skip first 10 points (usually noisy)\n",
    "    skip_end=5,       # Skip last 5 points (usually diverged)\n",
    "    suggest_lr=True   # Show suggested learning rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f70a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis and Export\n",
    "print(\"üî¨ Advanced Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if len(learning_rates) > 20:\n",
    "    # Skip noisy portions for analysis\n",
    "    start_idx = 10\n",
    "    end_idx = len(learning_rates) - 5\n",
    "    lr_analysis = learning_rates[start_idx:end_idx]\n",
    "    loss_analysis = losses[start_idx:end_idx]\n",
    "    \n",
    "    # Find key points\n",
    "    min_loss_idx = np.argmin(loss_analysis)\n",
    "    min_loss_lr = lr_analysis[min_loss_idx]\n",
    "    min_loss = loss_analysis[min_loss_idx]\n",
    "    \n",
    "    # Calculate gradients for steepest descent\n",
    "    gradients = np.gradient(loss_analysis)\n",
    "    steepest_idx = np.argmin(gradients)\n",
    "    steepest_lr = lr_analysis[steepest_idx]\n",
    "    \n",
    "    # Loss reduction analysis\n",
    "    initial_loss = loss_analysis[0]\n",
    "    max_reduction = initial_loss - min_loss\n",
    "    reduction_percent = (max_reduction / initial_loss) * 100\n",
    "    \n",
    "    print(f\"üìä Analysis Summary:\")\n",
    "    print(f\"   üî¥ Steepest descent LR: {steepest_lr:.2e}\")\n",
    "    print(f\"   üü¢ Minimum loss LR: {min_loss_lr:.2e}\")\n",
    "    print(f\"   üìâ Best loss achieved: {min_loss:.4f}\")\n",
    "    print(f\"   üìà Initial loss: {initial_loss:.4f}\")\n",
    "    print(f\"   üéØ Loss reduction: {reduction_percent:.1f}%\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    conservative_lr = steepest_lr / 10\n",
    "    aggressive_lr = steepest_lr * 2\n",
    "    \n",
    "    print(f\"\\nüí° Training Recommendations:\")\n",
    "    print(f\"   üü† Conservative start: {conservative_lr:.2e}\")\n",
    "    print(f\"   üî¥ Optimal range: {steepest_lr:.2e}\")\n",
    "    print(f\"   üî• Aggressive (risky): {aggressive_lr:.2e}\")\n",
    "    \n",
    "    # Dataset-specific advice\n",
    "    if MODEL_CONFIG['num_classes'] <= 10:\n",
    "        print(f\"\\nüéØ For small datasets ({MODEL_CONFIG['num_classes']} classes):\")\n",
    "        print(f\"   ‚Ä¢ Start conservative: {conservative_lr:.2e}\")\n",
    "        print(f\"   ‚Ä¢ Use learning rate scheduling\")\n",
    "        print(f\"   ‚Ä¢ Consider pretrained weights\")\n",
    "    elif MODEL_CONFIG['num_classes'] == 200:\n",
    "        print(f\"\\nüéØ For Tiny ImageNet (200 classes):\")\n",
    "        print(f\"   ‚Ä¢ Good starting LR: {steepest_lr:.2e}\")\n",
    "        print(f\"   ‚Ä¢ Use step or cosine LR scheduling\")\n",
    "        print(f\"   ‚Ä¢ Train for 50-100 epochs\")\n",
    "    else:\n",
    "        print(f\"\\nüéØ For large datasets ({MODEL_CONFIG['num_classes']} classes):\")\n",
    "        print(f\"   ‚Ä¢ Start with: {conservative_lr:.2e}\")\n",
    "        print(f\"   ‚Ä¢ Gradually increase if stable\")\n",
    "        print(f\"   ‚Ä¢ Use warmup for first few epochs\")\n",
    "    \n",
    "    # Export results\n",
    "    lr_finder.export_results('lr_finder_results.csv')\n",
    "    \n",
    "    print(f\"\\nüìÑ Results exported to lr_finder_results.csv\")\n",
    "    print(f\"\\nüöÄ Ready to start training with optimal learning rate!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data points for detailed analysis\")\n",
    "    print(\"Consider increasing num_iterations in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821d04d",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that you've found your optimal learning rate:\n",
    "\n",
    "### 1. üöÄ Start Training\n",
    "Go back to the main folder and use the suggested learning rate:\n",
    "\n",
    "```bash\n",
    "cd ..\n",
    "python train_imagenet.py --lr YOUR_OPTIMAL_LR\n",
    "```\n",
    "\n",
    "### 2. üìä Monitor Training\n",
    "- Watch the loss curves carefully\n",
    "- If loss increases rapidly, reduce LR\n",
    "- If training is too slow, slightly increase LR\n",
    "\n",
    "### 3. üîß Fine-tune\n",
    "- Use learning rate scheduling (StepLR, CosineAnnealingLR)\n",
    "- Consider warmup for the first few epochs\n",
    "- Adjust based on validation performance\n",
    "\n",
    "### 4. üéØ Dataset-Specific Tips\n",
    "- **Tiny ImageNet**: Start with found LR, train 50-100 epochs\n",
    "- **Imagenette/ImageWoof**: Use pretrained weights + found LR\n",
    "- **Full ImageNet**: Start conservative, use warmup\n",
    "\n",
    "Happy training! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
