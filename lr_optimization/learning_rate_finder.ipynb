{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f4c055",
   "metadata": {},
   "source": [
    "# Learning Rate Finder ðŸ”\n",
    "\n",
    "This notebook implements the Learning Rate Range Test (LR Finder) to help you find the optimal learning rate for training your ResNet50 model on ImageNet datasets.\n",
    "\n",
    "## What is Learning Rate Finder?\n",
    "\n",
    "The Learning Rate Finder is a technique that:\n",
    "- **Systematically tests** different learning rates during training\n",
    "- **Plots loss vs learning rate** to visualize the relationship\n",
    "- **Identifies the optimal range** where loss decreases most rapidly\n",
    "- **Prevents poor convergence** due to suboptimal learning rates\n",
    "\n",
    "## Key Benefits:\n",
    "- ðŸŽ¯ **Find optimal LR** - Discover the best learning rate for your specific setup\n",
    "- âš¡ **Faster convergence** - Start training with an ideal learning rate\n",
    "- ðŸ“Š **Visual guidance** - Clear plots show the optimal range\n",
    "- ðŸ›¡ï¸ **Avoid pitfalls** - Prevent too high/low learning rates\n",
    "\n",
    "## How to Use:\n",
    "1. Update dataset path and model configuration\n",
    "2. Run the learning rate finder\n",
    "3. Analyze the loss vs LR plot\n",
    "4. Choose the learning rate where loss decreases most steeply\n",
    "5. Use this LR for your actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "parent_dir = Path('../').resolve()\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "# Import our custom modules\n",
    "from imagenet_models import resnet50_imagenet\n",
    "from imagenet_dataset import get_imagenet_transforms\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“š Libraries imported successfully!\")\n",
    "print(f\"ðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ–¥ï¸ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Setup\n",
    "# âš ï¸ UPDATE THESE PATHS TO YOUR DATASET LOCATION\n",
    "IMAGENET_ROOT = \"../datasets/tiny-imagenet-200\"  # Default to Tiny ImageNet\n",
    "TRAIN_DIR = str(Path(IMAGENET_ROOT) / \"train\")\n",
    "VAL_DIR = str(Path(IMAGENET_ROOT) / \"val\")\n",
    "\n",
    "# Check if dataset exists, if not suggest alternatives\n",
    "if not Path(IMAGENET_ROOT).exists():\n",
    "    print(\"âš ï¸ Dataset not found at default location!\")\n",
    "    print(\"\\nðŸ“ Available dataset options:\")\n",
    "    datasets_dir = Path(\"../datasets\")\n",
    "    if datasets_dir.exists():\n",
    "        for dataset_path in datasets_dir.iterdir():\n",
    "            if dataset_path.is_dir():\n",
    "                print(f\"   â€¢ {dataset_path.name}\")\n",
    "    else:\n",
    "        print(\"   ðŸ“¥ No datasets found. Use dataset_tools/imagenet_subset_downloader.ipynb to download one!\")\n",
    "    \n",
    "    print(\"\\nðŸ”§ To use a different dataset, update IMAGENET_ROOT above\")\n",
    "else:\n",
    "    print(f\"âœ… Dataset found: {IMAGENET_ROOT}\")\n",
    "\n",
    "# Learning Rate Finder Configuration\n",
    "LR_FINDER_CONFIG = {\n",
    "    'min_lr': 1e-7,           # Minimum learning rate to test\n",
    "    'max_lr': 10.0,           # Maximum learning rate to test\n",
    "    'num_iterations': 100,    # Number of iterations to run\n",
    "    'beta': 0.98,             # Smoothing factor for loss\n",
    "    'stop_div_factor': 5,     # Stop if loss increases by this factor\n",
    "    'batch_size': 128,        # Batch size for LR finder\n",
    "    'num_workers': 4,         # Number of data loading workers\n",
    "    'subset_size': 5000,      # Use subset of data for faster testing\n",
    "}\n",
    "\n",
    "# Auto-detect dataset type and adjust configuration\n",
    "if \"tiny-imagenet\" in IMAGENET_ROOT.lower():\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 200,       # Tiny ImageNet has 200 classes\n",
    "        'pretrained': False,      # No pretrained for Tiny ImageNet\n",
    "        'input_size': 64,         # 64x64 images\n",
    "    }\n",
    "    LR_FINDER_CONFIG['batch_size'] = 256  # Can use larger batch for smaller images\n",
    "elif \"imagenette\" in IMAGENET_ROOT.lower() or \"imagewoof\" in IMAGENET_ROOT.lower():\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 10,        # Imagenette/ImageWoof have 10 classes\n",
    "        'pretrained': True,       # Pretrained recommended for smaller datasets\n",
    "        'input_size': 224,        # Standard ImageNet size\n",
    "    }\n",
    "else:\n",
    "    # Full ImageNet or custom dataset\n",
    "    MODEL_CONFIG = {\n",
    "        'model_name': 'resnet50',\n",
    "        'num_classes': 1000,      # Full ImageNet classes\n",
    "        'pretrained': False,      # Set to True if you want pretrained weights\n",
    "        'input_size': 224,        # Standard ImageNet size\n",
    "    }\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cpu':\n",
    "    print(\"âš ï¸ Warning: Using CPU. LR finder will be slow.\")\n",
    "    LR_FINDER_CONFIG['batch_size'] = 32  # Smaller batch for CPU\n",
    "    LR_FINDER_CONFIG['subset_size'] = 1000  # Smaller subset for CPU\n",
    "\n",
    "print(\"\\nðŸ“‹ LR Finder Configuration:\")\n",
    "for key, value in LR_FINDER_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ—ï¸ Model Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    \"\"\"Learning Rate Range Test Implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        # Store initial model and optimizer state\n",
    "        self.model_state = copy.deepcopy(model.state_dict())\n",
    "        self.optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "        \n",
    "        # Results storage\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset model and optimizer to initial state\"\"\"\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def find_lr(self, train_loader, min_lr=1e-7, max_lr=10.0, num_iterations=100, \n",
    "                beta=0.98, stop_div_factor=5):\n",
    "        \"\"\"Perform learning rate range test\"\"\"\n",
    "        \n",
    "        print(f\"ðŸ” Starting Learning Rate Finder...\")\n",
    "        print(f\"   ðŸ“Š LR range: {min_lr:.2e} to {max_lr:.2e}\")\n",
    "        print(f\"   ðŸ”„ Iterations: {num_iterations}\")\n",
    "        \n",
    "        # Reset to initial state\n",
    "        self.reset()\n",
    "        \n",
    "        # Calculate learning rate schedule\n",
    "        lr_schedule = np.logspace(np.log10(min_lr), np.log10(max_lr), num_iterations)\n",
    "        \n",
    "        # Training setup\n",
    "        self.model.train()\n",
    "        data_iter = iter(train_loader)\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        smoothed_loss = 0\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(enumerate(lr_schedule), total=num_iterations, desc=\"LR Finder\")\n",
    "        \n",
    "        for i, lr in pbar:\n",
    "            # Update learning rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # Get next batch (cycle through dataset if needed)\n",
    "            try:\n",
    "                inputs, targets = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                inputs, targets = next(data_iter)\n",
    "            \n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "            # Smooth loss for better visualization\n",
    "            if i == 0:\n",
    "                smoothed_loss = current_loss\n",
    "            else:\n",
    "                smoothed_loss = beta * smoothed_loss + (1 - beta) * current_loss\n",
    "            \n",
    "            # Store values\n",
    "            self.learning_rates.append(lr)\n",
    "            self.losses.append(smoothed_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'LR': f'{lr:.2e}',\n",
    "                'Loss': f'{smoothed_loss:.4f}'\n",
    "            })\n",
    "            \n",
    "            # Early stopping if loss explodes\n",
    "            if i > 0 and smoothed_loss > stop_div_factor * best_loss:\n",
    "                print(f\"\\nðŸ›‘ Stopping early at iteration {i}: loss exploded!\")\n",
    "                break\n",
    "            \n",
    "            # Track best loss\n",
    "            if smoothed_loss < best_loss:\n",
    "                best_loss = smoothed_loss\n",
    "        \n",
    "        pbar.close()\n",
    "        print(f\"âœ… Learning rate finder completed!\")\n",
    "        print(f\"   ðŸ“Š Tested {len(self.learning_rates)} learning rates\")\n",
    "        print(f\"   ðŸ“‰ Best loss: {best_loss:.4f}\")\n",
    "        \n",
    "        return self.learning_rates, self.losses\n",
    "    \n",
    "    def plot_results(self, skip_start=10, skip_end=5, suggest_lr=True):\n",
    "        \"\"\"Plot learning rate vs loss with suggestions\"\"\"\n",
    "        \n",
    "        if len(self.learning_rates) == 0:\n",
    "            print(\"âŒ No results to plot. Run find_lr() first!\")\n",
    "            return\n",
    "        \n",
    "        # Skip noisy start and end portions\n",
    "        start_idx = skip_start\n",
    "        end_idx = len(self.learning_rates) - skip_end\n",
    "        \n",
    "        lr_plot = self.learning_rates[start_idx:end_idx]\n",
    "        loss_plot = self.losses[start_idx:end_idx]\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Main plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.semilogx(lr_plot, loss_plot, 'b-', linewidth=2, label='Loss vs LR')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder Results', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Suggest optimal learning rate\n",
    "        if suggest_lr and len(loss_plot) > 10:\n",
    "            # Find steepest descent (negative gradient)\n",
    "            gradients = np.gradient(loss_plot)\n",
    "            min_gradient_idx = np.argmin(gradients)\n",
    "            suggested_lr = lr_plot[min_gradient_idx]\n",
    "            \n",
    "            # Also find minimum loss point for reference\n",
    "            min_loss_idx = np.argmin(loss_plot)\n",
    "            min_loss_lr = lr_plot[min_loss_idx]\n",
    "            \n",
    "            # Plot suggestions\n",
    "            plt.axvline(suggested_lr, color='red', linestyle='--', alpha=0.8, \n",
    "                       label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "            plt.axvline(min_loss_lr, color='green', linestyle='--', alpha=0.8,\n",
    "                       label=f'Min Loss LR: {min_loss_lr:.2e}')\n",
    "            \n",
    "            # Conservative suggestion (1/10 of steepest point)\n",
    "            conservative_lr = suggested_lr / 10\n",
    "            plt.axvline(conservative_lr, color='orange', linestyle='--', alpha=0.8,\n",
    "                       label=f'Conservative LR: {conservative_lr:.2e}')\n",
    "        \n",
    "        plt.legend()\n",
    "        \n",
    "        # Gradient plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        if len(loss_plot) > 1:\n",
    "            gradients = np.gradient(loss_plot)\n",
    "            plt.semilogx(lr_plot, gradients, 'r-', linewidth=2, label='Loss Gradient')\n",
    "            plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Loss Gradient')\n",
    "            plt.title('Loss Gradient (Choose LR where gradient is most negative)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print recommendations\n",
    "        if suggest_lr and len(loss_plot) > 10:\n",
    "            print(\"\\nðŸŽ¯ Learning Rate Recommendations:\")\n",
    "            print(f\"   ðŸ”´ Steepest descent: {suggested_lr:.2e} (where loss decreases fastest)\")\n",
    "            print(f\"   ðŸŸ¢ Minimum loss: {min_loss_lr:.2e} (lowest loss achieved)\")\n",
    "            print(f\"   ðŸŸ  Conservative: {conservative_lr:.2e} (safer choice, 1/10 of steepest)\")\n",
    "            print(\"\\nðŸ’¡ Recommended approach:\")\n",
    "            print(f\"   â€¢ Start with conservative LR: {conservative_lr:.2e}\")\n",
    "            print(f\"   â€¢ If training is stable, try steepest: {suggested_lr:.2e}\")\n",
    "            print(f\"   â€¢ Monitor loss and adjust as needed\")\n",
    "    \n",
    "    def export_results(self, filename='lr_finder_results.txt'):\n",
    "        \"\"\"Export results to file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"Learning Rate Finder Results\\n\")\n",
    "            f.write(\"===========================\\n\\n\")\n",
    "            for lr, loss in zip(self.learning_rates, self.losses):\n",
    "                f.write(f\"{lr:.6e}\\t{loss:.6f}\\n\")\n",
    "        print(f\"ðŸ“„ Results exported to {filename}\")\n",
    "\n",
    "print(\"ðŸ—ï¸ LearningRateFinder class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset and DataLoader\n",
    "print(\"ðŸ“Š Preparing dataset...\")\n",
    "\n",
    "# Get transforms based on input size\n",
    "if MODEL_CONFIG['input_size'] == 64:\n",
    "    # Tiny ImageNet transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "else:\n",
    "    # Standard ImageNet transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=TRAIN_DIR,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"   ðŸ“Š Total samples: {len(train_dataset):,}\")\n",
    "    print(f\"   ðŸ·ï¸ Number of classes: {len(train_dataset.classes)}\")\n",
    "    \n",
    "    # Create subset for faster LR finding\n",
    "    subset_size = min(LR_FINDER_CONFIG['subset_size'], len(train_dataset))\n",
    "    subset_indices = torch.randperm(len(train_dataset))[:subset_size]\n",
    "    train_subset = Subset(train_dataset, subset_indices)\n",
    "    \n",
    "    print(f\"   ðŸŽ¯ Using subset: {len(train_subset):,} samples\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=LR_FINDER_CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=LR_FINDER_CONFIG['num_workers'],\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"   ðŸ“¦ Batch size: {LR_FINDER_CONFIG['batch_size']}\")\n",
    "    print(f\"   ðŸ”„ Number of batches: {len(train_loader)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Check if dataset path is correct\")\n",
    "    print(\"   2. Ensure dataset has train/ subdirectory\")\n",
    "    print(\"   3. Verify dataset structure (train/class_name/images.jpg)\")\n",
    "    print(\"   4. Download dataset using: ../dataset_tools/imagenet_subset_downloader.ipynb\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model, Optimizer, and Loss Function\n",
    "print(\"ðŸ—ï¸ Initializing model...\")\n",
    "\n",
    "# Create model\n",
    "model = resnet50_imagenet(\n",
    "    num_classes=MODEL_CONFIG['num_classes'],\n",
    "    pretrained=MODEL_CONFIG['pretrained']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model created: {MODEL_CONFIG['model_name']}\")\n",
    "print(f\"   ðŸŽ¯ Classes: {MODEL_CONFIG['num_classes']}\")\n",
    "print(f\"   ðŸ”„ Pretrained: {MODEL_CONFIG['pretrained']}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   ðŸ“Š Total parameters: {total_params:,}\")\n",
    "print(f\"   ðŸŽ“ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Initialize optimizer (SGD is commonly used for LR finding)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LR_FINDER_CONFIG['min_lr'],  # Will be updated during LR finding\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"âœ… Optimizer: SGD with momentum=0.9, weight_decay=1e-4\")\n",
    "print(f\"âœ… Loss function: CrossEntropyLoss\")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        # Create dummy input based on input size\n",
    "        dummy_input = torch.randn(2, 3, MODEL_CONFIG['input_size'], MODEL_CONFIG['input_size']).to(device)\n",
    "        dummy_output = model(dummy_input)\n",
    "        print(f\"âœ… Forward pass test successful!\")\n",
    "        print(f\"   ðŸ“Š Input shape: {dummy_input.shape}\")\n",
    "        print(f\"   ðŸ“Š Output shape: {dummy_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Forward pass test failed: {e}\")\n",
    "        raise\n",
    "\n",
    "model.train()  # Set back to training mode\n",
    "print(\"\\nðŸš€ Ready for learning rate finding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning Rate Finder\n",
    "print(\"ðŸ” Running Learning Rate Finder...\")\n",
    "print(\"\" * 50)\n",
    "\n",
    "# Create LR Finder instance\n",
    "lr_finder = LearningRateFinder(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Run the learning rate range test\n",
    "learning_rates, losses = lr_finder.find_lr(\n",
    "    train_loader=train_loader,\n",
    "    min_lr=LR_FINDER_CONFIG['min_lr'],\n",
    "    max_lr=LR_FINDER_CONFIG['max_lr'],\n",
    "    num_iterations=LR_FINDER_CONFIG['num_iterations'],\n",
    "    beta=LR_FINDER_CONFIG['beta'],\n",
    "    stop_div_factor=LR_FINDER_CONFIG['stop_div_factor']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸŽ‰ Learning Rate Finder completed!\")\n",
    "print(f\"ðŸ“Š Tested {len(learning_rates)} learning rates\")\n",
    "print(f\"ðŸ“‰ Loss range: {min(losses):.4f} to {max(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7efa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and Analyze Results\n",
    "print(\"ðŸ“Š Plotting results and analyzing optimal learning rate...\")\n",
    "\n",
    "# Plot the results with suggestions\n",
    "lr_finder.plot_results(\n",
    "    skip_start=10,    # Skip first 10 points (usually noisy)\n",
    "    skip_end=5,       # Skip last 5 points (usually diverged)\n",
    "    suggest_lr=True   # Show suggested learning rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f70a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis and Export\n",
    "print(\"ðŸ”¬ Advanced Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if len(learning_rates) > 20:\n",
    "    # Skip noisy portions for analysis\n",
    "    start_idx = 10\n",
    "    end_idx = len(learning_rates) - 5\n",
    "    lr_analysis = learning_rates[start_idx:end_idx]\n",
    "    loss_analysis = losses[start_idx:end_idx]\n",
    "    \n",
    "    # Find key points\n",
    "    min_loss_idx = np.argmin(loss_analysis)\n",
    "    min_loss_lr = lr_analysis[min_loss_idx]\n",
    "    min_loss = loss_analysis[min_loss_idx]\n",
    "    \n",
    "    # Calculate gradients for steepest descent\n",
    "    gradients = np.gradient(loss_analysis)\n",
    "    steepest_idx = np.argmin(gradients)\n",
    "    steepest_lr = lr_analysis[steepest_idx]\n",
    "    \n",
    "    # Loss reduction analysis\n",
    "    initial_loss = loss_analysis[0]\n",
    "    max_reduction = initial_loss - min_loss\n",
    "    reduction_percent = (max_reduction / initial_loss) * 100\n",
    "    \n",
    "    print(f\"ðŸ“Š Analysis Summary:\")\n",
    "    print(f\"   ðŸ”´ Steepest descent LR: {steepest_lr:.2e}\")\n",
    "    print(f\"   ðŸŸ¢ Minimum loss LR: {min_loss_lr:.2e}\")\n",
    "    print(f\"   ðŸ“‰ Best loss achieved: {min_loss:.4f}\")\n",
    "    print(f\"   ðŸ“ˆ Initial loss: {initial_loss:.4f}\")\n",
    "    print(f\"   ðŸŽ¯ Loss reduction: {reduction_percent:.1f}%\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    conservative_lr = steepest_lr / 10\n",
    "    aggressive_lr = steepest_lr * 2\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Training Recommendations:\")\n",
    "    print(f\"   ðŸŸ  Conservative start: {conservative_lr:.2e}\")\n",
    "    print(f\"   ðŸ”´ Optimal range: {steepest_lr:.2e}\")\n",
    "    print(f\"   ðŸ”¥ Aggressive (risky): {aggressive_lr:.2e}\")\n",
    "    \n",
    "    # Dataset-specific advice\n",
    "    if MODEL_CONFIG['num_classes'] <= 10:\n",
    "        print(f\"\\nðŸŽ¯ For small datasets ({MODEL_CONFIG['num_classes']} classes):\")\n",
    "        print(f\"   â€¢ Start conservative: {conservative_lr:.2e}\")\n",
    "        print(f\"   â€¢ Use learning rate scheduling\")\n",
    "        print(f\"   â€¢ Consider pretrained weights\")\n",
    "    elif MODEL_CONFIG['num_classes'] == 200:\n",
    "        print(f\"\\nðŸŽ¯ For Tiny ImageNet (200 classes):\")\n",
    "        print(f\"   â€¢ Good starting LR: {steepest_lr:.2e}\")\n",
    "        print(f\"   â€¢ Use step or cosine LR scheduling\")\n",
    "        print(f\"   â€¢ Train for 50-100 epochs\")\n",
    "    else:\n",
    "        print(f\"\\nðŸŽ¯ For large datasets ({MODEL_CONFIG['num_classes']} classes):\")\n",
    "        print(f\"   â€¢ Start with: {conservative_lr:.2e}\")\n",
    "        print(f\"   â€¢ Gradually increase if stable\")\n",
    "        print(f\"   â€¢ Use warmup for first few epochs\")\n",
    "    \n",
    "    # Export results\n",
    "    lr_finder.export_results('lr_finder_results.csv')\n",
    "    \n",
    "    print(f\"\\nðŸ“„ Results exported to lr_finder_results.csv\")\n",
    "    print(f\"\\nðŸš€ Ready to start training with optimal learning rate!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Not enough data points for detailed analysis\")\n",
    "    print(\"Consider increasing num_iterations in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821d04d",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "Now that you've found your optimal learning rate:\n",
    "\n",
    "### 1. ðŸš€ Start Training\n",
    "Go back to the main folder and use the suggested learning rate:\n",
    "\n",
    "```bash\n",
    "cd ..\n",
    "python train_imagenet.py --lr YOUR_OPTIMAL_LR\n",
    "```\n",
    "\n",
    "### 2. ðŸ“Š Monitor Training\n",
    "- Watch the loss curves carefully\n",
    "- If loss increases rapidly, reduce LR\n",
    "- If training is too slow, slightly increase LR\n",
    "\n",
    "### 3. ðŸ”§ Fine-tune\n",
    "- Use learning rate scheduling (StepLR, CosineAnnealingLR)\n",
    "- Consider warmup for the first few epochs\n",
    "- Adjust based on validation performance\n",
    "\n",
    "### 4. ðŸŽ¯ Dataset-Specific Tips\n",
    "- **Tiny ImageNet**: Start with found LR, train 50-100 epochs\n",
    "- **Imagenette/ImageWoof**: Use pretrained weights + found LR\n",
    "- **Full ImageNet**: Start conservative, use warmup\n",
    "\n",
    "Happy training! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
